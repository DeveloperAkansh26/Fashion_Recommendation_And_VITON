{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12533502,"sourceType":"datasetVersion","datasetId":7912190},{"sourceId":12533533,"sourceType":"datasetVersion","datasetId":7912217},{"sourceId":12535029,"sourceType":"datasetVersion","datasetId":7913325},{"sourceId":12535142,"sourceType":"datasetVersion","datasetId":7913394},{"sourceId":3848,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":2749,"modelId":324}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/GoGoDuck912/Self-Correction-Human-Parsing.git\n%cd Self-Correction-Human-Parsing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:22:15.727520Z","iopub.execute_input":"2025-07-21T13:22:15.727817Z","iopub.status.idle":"2025-07-21T13:22:16.699034Z","shell.execute_reply.started":"2025-07-21T13:22:15.727794Z","shell.execute_reply":"2025-07-21T13:22:16.698246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# Create the directory if it doesn't exist\nos.makedirs(\"/kaggle/working/human-parsing\", exist_ok=True)\n\nimport os\n\n# Create the directory if it doesn't exist\nos.makedirs(\"/kaggle/working/cloth-mask\", exist_ok=True)\n\nimport os\n\n# Create the directory if it doesn't exist\nos.makedirs(\"/kaggle/working/json_keypoints\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:22:18.005864Z","iopub.execute_input":"2025-07-21T13:22:18.006148Z","iopub.status.idle":"2025-07-21T13:22:18.011302Z","shell.execute_reply.started":"2025-07-21T13:22:18.006121Z","shell.execute_reply":"2025-07-21T13:22:18.010501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install the modern library and other dependencies\n!pip install inplace-abn\n!pip install -r requirements.txt\n\n# --- CRITICAL FIXES ---\n\n# 1. Delete the repository's outdated 'modules' folder\nprint(\"Deleting outdated 'modules' folder...\")\n!rm -rf ./modules\n\n# 2. Edit the script to import the modern library instead\nprint(\"Fixing import statement...\")\n!sed -i 's/from modules import InPlaceABNSync/from inplace_abn import InPlaceABNSync/' ./networks/AugmentCE2P.py\n\n# 3. Fix the incompatible activation function name\nprint(\"Fixing activation function name...\")\n!sed -i \"s/activation='none'/activation='identity'/g\" ./networks/AugmentCE2P.py\n\nprint(\"\\nAll fixes applied successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:22:20.324935Z","iopub.execute_input":"2025-07-21T13:22:20.325184Z","iopub.status.idle":"2025-07-21T13:25:10.552339Z","shell.execute_reply.started":"2025-07-21T13:22:20.325166Z","shell.execute_reply":"2025-07-21T13:25:10.551537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/facebookresearch/segment-anything.git\n!pip install opencv-python matplotlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:25:10.553744Z","iopub.execute_input":"2025-07-21T13:25:10.554007Z","iopub.status.idle":"2025-07-21T13:25:19.352086Z","shell.execute_reply.started":"2025-07-21T13:25:10.553982Z","shell.execute_reply":"2025-07-21T13:25:19.351267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tfjs-graph-converter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:25:19.352986Z","iopub.execute_input":"2025-07-21T13:25:19.353246Z","iopub.status.idle":"2025-07-21T13:25:23.397921Z","shell.execute_reply.started":"2025-07-21T13:25:19.353220Z","shell.execute_reply":"2025-07-21T13:25:23.397201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/rwightman/posenet-pytorch  # fix bugs\n%cd posenet-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:25:23.399707Z","iopub.execute_input":"2025-07-21T13:25:23.399929Z","iopub.status.idle":"2025-07-21T13:25:23.921456Z","shell.execute_reply.started":"2025-07-21T13:25:23.399906Z","shell.execute_reply":"2025-07-21T13:25:23.920599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parsing_map(image_dir, output_dir=\"/kaggle/working/human-parsing\"):\n    \"\"\"\n    Runs the human parsing script from the correct directory.\n    \"\"\"\n    # This command assumes you've already run the setup cell that clones the repo\n    # and installs dependencies.\n    %cd /kaggle/working/Self-Correction-Human-Parsing\n    \n    !python3 simple_extractor.py \\\n        --dataset lip \\\n        --model-restore /kaggle/input/lip-modellll/exp-schp-201908261155-lip.pth \\\n        --input-dir {image_dir} \\\n        --output-dir {output_dir}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:27:41.508653Z","iopub.execute_input":"2025-07-21T13:27:41.509323Z","iopub.status.idle":"2025-07-21T13:27:41.514817Z","shell.execute_reply.started":"2025-07-21T13:27:41.509294Z","shell.execute_reply":"2025-07-21T13:27:41.513982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"parsing_map('/kaggle/input/data-of-user/data/image')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:27:45.203735Z","iopub.execute_input":"2025-07-21T13:27:45.204361Z","iopub.status.idle":"2025-07-21T13:27:52.872732Z","shell.execute_reply.started":"2025-07-21T13:27:45.204334Z","shell.execute_reply":"2025-07-21T13:27:52.871797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CLOTH MASK","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom segment_anything import sam_model_registry, SamPredictor\nimport os\n\ndef cloth_mask(cloth_path, output_dir=\"/kaggle/working/cloth-mask\"):\n    \"\"\"\n    Creates a segmentation mask for a single cloth image.\n    \"\"\"\n    # (SAM model loading logic is the same...)\n    sam_checkpoint = \"/kaggle/input/segment-anything/pytorch/vit-b/1/model.pth\"\n    model_type = \"vit_b\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)\n    predictor = SamPredictor(sam)\n\n    image = cv2.imread(cloth_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    predictor.set_image(image)\n    \n    h, w = image.shape[:2]\n    input_point = np.array([[w // 2, h // 2]])\n    input_label = np.array([1])\n    \n    masks, scores, _ = predictor.predict(\n        point_coords=input_point, point_labels=input_label, multimask_output=True)\n        \n    best_mask = (masks[np.argmax(scores)] * 255).astype(np.uint8)\n    \n    # <-- FIX: Create a dynamic output name.\n    os.makedirs(output_dir, exist_ok=True)\n    base_name = os.path.basename(cloth_path)\n    file_name_without_ext = os.path.splitext(base_name)[0]\n    output_filename = f\"{file_name_without_ext}_mask.png\"\n    output_path = os.path.join(output_dir, output_filename)\n    \n    cv2.imwrite(output_path, best_mask)\n    print(f\"Saved cloth mask to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:25:23.927924Z","iopub.execute_input":"2025-07-21T13:25:23.928112Z","iopub.status.idle":"2025-07-21T13:25:31.042452Z","shell.execute_reply.started":"2025-07-21T13:25:23.928096Z","shell.execute_reply":"2025-07-21T13:25:31.041701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now, calling the function will work correctly\ncloth_mask('/kaggle/input/data-of-user/data/cloth/00000_00.jpg')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-21T13:21:40.049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nimport posenet\nfrom posenet.models.model_factory import load_model\n\ndef keypoints(image_path, output_dir=\"/kaggle/working/json_keypoints\"):\n    \"\"\"\n    Generates pose keypoints for a single image and saves them to a JSON file.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    net = load_model(101).cuda()\n    output_stride = net.output_stride\n    \n    input_image, _, _ = posenet.read_imgfile(\n        image_path, scale_factor=1.0, output_stride=output_stride)\n\n    with torch.no_grad():\n        input_image = torch.Tensor(input_image).cuda()\n        heatmaps, offsets, fwd, bwd = net(input_image)\n        pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multiple_poses(\n            heatmaps.squeeze(0), offsets.squeeze(0), fwd.squeeze(0), bwd.squeeze(0),\n            output_stride=output_stride, max_pose_detections=1, min_pose_score=0.25)\n    \n    poses = keypoint_coords.astype(np.int32)\n    if not len(poses) > 0:\n        print(f\"Warning: No pose detected for {image_path}\")\n        return None\n\n    pose = poses[0]\n    \n    indices = [0, (5, 6), 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n    openpose = []\n    for ix in indices:\n        if ix == (5, 6):\n            openpose.append([int((pose[5][1] + pose[6][1]) / 2), int((pose[5][0] + pose[6][0]) / 2), 1])\n        else:\n            openpose.append([int(pose[ix][1]), int(pose[ix][0]), 1])\n\n    coords = [float(item) for sublist in openpose for item in sublist]\n    data = {\"people\": [{\"pose_keypoints_2d\": coords}]}\n    \n    base_name = os.path.basename(image_path)\n    file_name_without_ext = os.path.splitext(base_name)[0]\n    output_filename = f\"{file_name_without_ext}_keypoints.json\"\n    output_path = os.path.join(output_dir, output_filename)\n    \n    with open(output_path, 'w') as f:\n        json.dump(data, f)\n    \n    return output_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:28:37.128670Z","iopub.execute_input":"2025-07-21T13:28:37.129363Z","iopub.status.idle":"2025-07-21T13:28:37.139134Z","shell.execute_reply.started":"2025-07-21T13:28:37.129332Z","shell.execute_reply":"2025-07-21T13:28:37.138439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keypoints('/kaggle/input/data-of-user/data/image/00009_00.jpg')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-21T13:21:40.049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport json\nimport numpy as np\nimport os # <-- Added the missing import\n\ndef open_pose(image_path, keypoints_path, output_dir=\"/kaggle/working/openpose_image\"):\n    \"\"\"\n    Draws a pose skeleton on a blank canvas using keypoints from a JSON file.\n\n    Args:\n        image_path (str): Path to the original image to get canvas dimensions.\n        keypoints_path (str): Path to the JSON file containing the pose keypoints.\n        output_dir (str): Directory where the output skeleton image will be saved.\n    \"\"\"\n    # --- Load image using the function argument ---\n    img = cv2.imread(image_path)\n    if img is None:\n        print(f\"Error: Could not load image from {image_path}\")\n        return\n        \n    canvas_height, canvas_width = img.shape[:2]\n    canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)\n    \n    # --- Load keypoints using the function argument ---\n    with open(keypoints_path) as f:\n        keypoints_json = json.load(f)\n    \n    # Check if keypoints were found\n    if not keypoints_json['people']:\n        print(f\"Error: No people found in keypoints file {keypoints_path}\")\n        return\n\n    keypoints = np.array(keypoints_json['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n    \n    # --- Define pose pairs ---\n    POSE_PAIRS = [\n        (1, 2), (1, 5), (2, 3), (3, 4), (5, 6),\n        (6, 7), (1, 8), (8, 9), (9, 10), (1, 11),\n        (11, 12), (12, 13), (0, 1), (0, 14),\n        (14, 16), (0, 15), (15, 17)\n    ]\n    \n    # --- Color gradient for limbs (from red to violet) ---\n    def get_color(index, total):\n        hsv = np.array([[[int(index / total * 180), 255, 255]]], dtype=np.uint8)\n        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)[0][0]\n        return tuple(int(c) for c in bgr)\n    \n    # --- Draw keypoints, labels, and colored limbs ---\n    for i, (x, y, conf) in enumerate(keypoints):\n        if conf > 0.1:\n            cv2.circle(canvas, (int(x), int(y)), 5, (255, 255, 255), -1)  # white joint\n            cv2.putText(canvas, str(i), (int(x)+5, int(y)-5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n    \n    for i, (partA, partB) in enumerate(POSE_PAIRS):\n        xA, yA, cA = keypoints[partA]\n        xB, yB, cB = keypoints[partB]\n        if cA > 0.1 and cB > 0.1:\n            ptA = (int(xA), int(yA))\n            ptB = (int(xB), int(yB))\n            color = get_color(i, len(POSE_PAIRS))  # rainbow color\n            cv2.line(canvas, ptA, ptB, color, thickness=10)\n    \n    # --- Save the final skeleton image ---\n    os.makedirs(output_dir, exist_ok=True)\n    # Create a dynamic output name based on the input image\n    base_name = os.path.basename(image_path)\n    file_name_without_ext = os.path.splitext(base_name)[0]\n    output_filename = f\"{file_name_without_ext}_skeleton.png\"\n    output_path = os.path.join(output_dir, output_filename)\n    \n    cv2.imwrite(output_path, canvas)\n    print(f\"✅ Skeleton image saved to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:25:31.059778Z","iopub.execute_input":"2025-07-21T13:25:31.059985Z","iopub.status.idle":"2025-07-21T13:25:31.077698Z","shell.execute_reply.started":"2025-07-21T13:25:31.059969Z","shell.execute_reply":"2025-07-21T13:25:31.077209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef pre_processing(cloth_dir, image_dir):\n    \"\"\"\n    Runs the full pre-processing pipeline by pairing sorted files from\n    the cloth and image directories.\n    \"\"\"\n    print(\"--- Starting Full Pre-processing Pipeline ---\")\n    \n    # Step 1: Run parsing on the entire image directory\n    print(\"\\n[1/4] Running human parsing on all images...\")\n    parsing_map(image_dir) \n    \n    image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n    cloth_files = sorted([f for f in os.listdir(cloth_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n\n    if len(image_files) != len(cloth_files):\n        print(f\"Warning: Mismatch in file counts. Images: {len(image_files)}, Cloths: {len(cloth_files)}\")\n        print(\"Processing only the minimum number of pairs.\")\n        \n    num_pairs = min(len(image_files), len(cloth_files))\n    \n    # Loop through the number of pairs\n    for i in range(num_pairs):\n        image_filename = image_files[i]\n        cloth_filename = cloth_files[i]\n        \n        print(f\"\\n--- Processing Pair {i+1}/{num_pairs}: ---\")\n        print(f\"  Image: {image_filename}\")\n        print(f\"  Cloth: {cloth_filename}\")\n        \n        # Define paths for the current pair\n        image_path = os.path.join(image_dir, image_filename)\n        cloth_path = os.path.join(cloth_dir, cloth_filename)\n\n        # Step 2: Create cloth mask\n        print(f\"[2/4] Creating cloth mask for {cloth_filename}...\")\n        cloth_mask(cloth_path)\n\n        # Step 3: Create keypoints JSON\n        print(f\"[3/4] Creating keypoints for {image_filename}...\")\n        keypoints_json_path = keypoints(image_path)\n        \n        if keypoints_json_path is None:\n            print(\"Skipping skeleton creation due to no keypoints.\")\n            continue\n\n        # Step 4: Create OpenPose skeleton image\n        print(f\"[4/4] Creating skeleton image for {image_filename}...\")\n        open_pose(image_path, keypoints_json_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:50:27.631070Z","iopub.execute_input":"2025-07-21T13:50:27.631346Z","iopub.status.idle":"2025-07-21T13:50:27.639678Z","shell.execute_reply.started":"2025-07-21T13:50:27.631316Z","shell.execute_reply":"2025-07-21T13:50:27.638819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define your input directories\nmy_cloth_directory = \"/kaggle/input/data-of-user/data/cloth\"\nmy_image_directory = \"/kaggle/input/dataset-of-person/data\"\n\n# Run the entire pipeline\npre_processing(cloth_dir=my_cloth_directory, image_dir=my_image_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T13:50:31.778313Z","iopub.execute_input":"2025-07-21T13:50:31.778593Z","iopub.status.idle":"2025-07-21T13:50:38.848339Z","shell.execute_reply.started":"2025-07-21T13:50:31.778570Z","shell.execute_reply":"2025-07-21T13:50:38.847315Z"}},"outputs":[],"execution_count":null}]}