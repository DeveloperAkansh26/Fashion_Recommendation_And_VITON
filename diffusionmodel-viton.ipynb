{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a0f55c5a-91cb-4ccc-baa4-f9bcb4305b02",
    "_uuid": "68a19c06-e6bc-4cf7-b62e-30d190c0541a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:31.967164Z",
     "iopub.status.busy": "2025-07-20T11:53:31.966877Z",
     "iopub.status.idle": "2025-07-20T11:53:42.126516Z",
     "shell.execute_reply": "2025-07-20T11:53:42.125887Z",
     "shell.execute_reply.started": "2025-07-20T11:53:31.967119Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 11:53:38.267625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753012418.291241     292 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753012418.298309     292 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchvision.transforms import transforms\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DPMSolverMultistepScheduler\n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "import google.generativeai as genai\n",
    "from transformers import AutoModel, AutoImageProcessor, CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f6d91f10-3dd4-473f-8460-c63faabf0d87",
    "_uuid": "9221f3a0-8de1-4c56-9487-b6fe4484a1a1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:42.128023Z",
     "iopub.status.busy": "2025-07-20T11:53:42.127526Z",
     "iopub.status.idle": "2025-07-20T11:53:42.402283Z",
     "shell.execute_reply": "2025-07-20T11:53:42.401675Z",
     "shell.execute_reply.started": "2025-07-20T11:53:42.128003Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_Token\"] = \"\"\n",
    "login(token=os.environ[\"HF_Token\"])\n",
    "\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "gemini = genai.GenerativeModel(\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "da9c3c44-7be0-4878-9428-0bf7ce241fb9",
    "_uuid": "1093e5df-52ca-431c-bec5-2f1a3a79286b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:42.403709Z",
     "iopub.status.busy": "2025-07-20T11:53:42.402975Z",
     "iopub.status.idle": "2025-07-20T11:53:42.407655Z",
     "shell.execute_reply": "2025-07-20T11:53:42.406926Z",
     "shell.execute_reply.started": "2025-07-20T11:53:42.403682Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "35d195ee-842f-45c3-afee-03b549a858d8",
    "_uuid": "0932497d-63cc-484e-bd07-693368ff4983",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:42.409511Z",
     "iopub.status.busy": "2025-07-20T11:53:42.409314Z",
     "iopub.status.idle": "2025-07-20T11:53:42.423184Z",
     "shell.execute_reply": "2025-07-20T11:53:42.422482Z",
     "shell.execute_reply.started": "2025-07-20T11:53:42.409495Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-2-base\"\n",
    "torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "85e8576f-abb3-4a8d-b3f6-1b40b280fe68",
    "_uuid": "5b47cd0b-8285-4c56-899f-3243e373acf3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:42.424164Z",
     "iopub.status.busy": "2025-07-20T11:53:42.423928Z",
     "iopub.status.idle": "2025-07-20T11:53:42.438234Z",
     "shell.execute_reply": "2025-07-20T11:53:42.437536Z",
     "shell.execute_reply.started": "2025-07-20T11:53:42.424119Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7e6df4d96ff0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d9356af5-2c73-4224-af32-532a82a4fe11",
    "_uuid": "3a72295a-83d5-4f63-a4e7-dfa6bfdd3f5d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "cfac26e2-ee59-4615-a202-22e88971983c",
    "_uuid": "8c20cd65-a47b-4a0b-9d73-d00896c9d0dc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:42.439193Z",
     "iopub.status.busy": "2025-07-20T11:53:42.438948Z",
     "iopub.status.idle": "2025-07-20T11:53:42.957507Z",
     "shell.execute_reply": "2025-07-20T11:53:42.956710Z",
     "shell.execute_reply.started": "2025-07-20T11:53:42.439173Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "837acfe7-ad34-4d7f-8aec-1fbb235de00b",
    "_uuid": "9b3ca269-8417-459b-8c21-adac75e22e2c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:42.958444Z",
     "iopub.status.busy": "2025-07-20T11:53:42.958257Z",
     "iopub.status.idle": "2025-07-20T11:53:42.969446Z",
     "shell.execute_reply": "2025-07-20T11:53:42.968541Z",
     "shell.execute_reply.started": "2025-07-20T11:53:42.958429Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, folder_path):\n",
    "        \n",
    "        self.image = os.path.join(folder_path, \"image\")\n",
    "        self.cloth = os.path.join(folder_path, \"cloth\")\n",
    "        self.cloth_mask = os.path.join(folder_path, \"cloth-mask\")\n",
    "        self.segmentation = os.path.join(folder_path, \"image-parse-v3\")\n",
    "        self.agnostic = os.path.join(folder_path, \"agnostic-v3.2\")\n",
    "\n",
    "        self.image_names = sorted([f for f in os.listdir(self.image)])\n",
    "        self.image_names.sort(key=lambda f: int(f.split(\"_\")[0]))\n",
    "\n",
    "        self.cloth_names = sorted([f for f in os.listdir(self.cloth)])\n",
    "        self.cloth_names.sort(key=lambda f: int(f.split(\"_\")[0]))\n",
    "\n",
    "        self.cloth_mask_names = sorted([f for f in os.listdir(self.cloth_mask)])\n",
    "        self.cloth_mask_names.sort(key=lambda f: int(f.split(\"_\")[0]))\n",
    "\n",
    "        self.segmentation_names = sorted([f for f in os.listdir(self.segmentation)])\n",
    "        self.segmentation_names.sort(key=lambda f: int(f.split(\"_\")[0]))\n",
    "\n",
    "        self.agnostic_names = sorted([f for f in os.listdir(self.agnostic)])\n",
    "        self.agnostic_names.sort(key=lambda f: int(f.split(\"_\")[0]))\n",
    "    \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        with open(\"/kaggle/input/prompts/generated_prompts.json\") as file:\n",
    "            self.generated_prompts = json.load(file)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 6506\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = Image.open(os.path.join(self.image, self.image_names[index]))\n",
    "        cloth = Image.open(os.path.join(self.cloth, self.cloth_names[index]))\n",
    "        cloth_mask = Image.open(os.path.join(self.cloth_mask, self.cloth_mask_names[index]))\n",
    "        segmentation = Image.open(os.path.join(self.segmentation, self.segmentation_names[index]))\n",
    "        agnostic = Image.open(os.path.join(self.agnostic, self.agnostic_names[index]))\n",
    "\n",
    "        prompt = self.generated_prompts[self.cloth_names[index]]\n",
    "        \n",
    "        text_input_ids = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "        \n",
    "        image = self.transform(image).to(torch_dtype).to(device)\n",
    "        cloth = self.transform(cloth).to(torch_dtype).to(device)\n",
    "        cloth_mask = self.mask_transform(cloth_mask).to(torch_dtype).to(device)\n",
    "        segmentation = self.mask_transform(segmentation).to(torch_dtype).to(device)\n",
    "        agnostic = self.transform(agnostic).to(torch_dtype).to(device)\n",
    "\n",
    "        return text_input_ids.to(device), cloth, cloth_mask, segmentation, agnostic, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "45c69e46-34f9-466d-8afc-c270e8673be9",
    "_uuid": "d8633283-c181-42ad-9073-2fef89e138b6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:42.970508Z",
     "iopub.status.busy": "2025-07-20T11:53:42.970292Z",
     "iopub.status.idle": "2025-07-20T11:53:43.055030Z",
     "shell.execute_reply": "2025-07-20T11:53:43.054259Z",
     "shell.execute_reply.started": "2025-07-20T11:53:42.970490Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\"/kaggle/input/vton-hd/train\")\n",
    "test_dataset = CustomDataset(\"/kaggle/input/vton-hd/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "9278143e-9020-4dcb-8af4-3ef9a01286c6",
    "_uuid": "3400a97b-d61e-4e44-a234-dc435e28f5e0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:43.056213Z",
     "iopub.status.busy": "2025-07-20T11:53:43.055940Z",
     "iopub.status.idle": "2025-07-20T11:53:43.060234Z",
     "shell.execute_reply": "2025-07-20T11:53:43.059537Z",
     "shell.execute_reply.started": "2025-07-20T11:53:43.056190Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eee71ea0-91ff-4f40-a83e-71fbff5b1307",
    "_uuid": "5d4e2328-7fc3-4026-9291-2fbb430b9231",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9cd4c641-4852-4914-8d52-280ca8087127",
    "_uuid": "0b5c8282-a3d1-4be5-8fee-0f3433e319da",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "def1d58f-a72b-466f-9ede-6f1ecc660b84",
    "_uuid": "0be3b340-f736-4518-b096-05f41fa14571",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:43.062696Z",
     "iopub.status.busy": "2025-07-20T11:53:43.062387Z",
     "iopub.status.idle": "2025-07-20T11:53:47.692188Z",
     "shell.execute_reply": "2025-07-20T11:53:47.691560Z",
     "shell.execute_reply.started": "2025-07-20T11:53:43.062656Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch_dtype).to(device)\n",
    "vae.requires_grad_(False)\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\", torch_dtype=torch_dtype).to(device)\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch_dtype).to(device)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "scheduler = DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "29b1a884-4576-43fc-ab31-f9803cd15718",
    "_uuid": "fbe2e8e3-323b-4867-92b3-1ffb7ec81084",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:47.693178Z",
     "iopub.status.busy": "2025-07-20T11:53:47.692903Z",
     "iopub.status.idle": "2025-07-20T11:53:47.697910Z",
     "shell.execute_reply": "2025-07-20T11:53:47.697016Z",
     "shell.execute_reply.started": "2025-07-20T11:53:47.693153Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomUNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(12, 4, kernel_size=(1, 1), stride=1)\n",
    "        self.unet = unet\n",
    "\n",
    "    def forward(self, latent_model_input, timesteps, text_embeddings):\n",
    "\n",
    "        latent_model_input = self.conv(latent_model_input)\n",
    "        model_pred = self.unet(latent_model_input, timesteps, encoder_hidden_states=text_embeddings)\n",
    "\n",
    "        return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "93d91944-dd2c-421a-b356-cee011a1a530",
    "_uuid": "03b4338d-2447-4902-b0b3-5d9dc5ff056d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:47.698903Z",
     "iopub.status.busy": "2025-07-20T11:53:47.698716Z",
     "iopub.status.idle": "2025-07-20T11:53:47.725849Z",
     "shell.execute_reply": "2025-07-20T11:53:47.725277Z",
     "shell.execute_reply.started": "2025-07-20T11:53:47.698889Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "custom_diff_model = CustomUNet()\n",
    "custom_diff_model = custom_diff_model.to(torch_dtype).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b9784abb-71f0-4e0c-b72f-d29465447940",
    "_uuid": "39365654-30f6-4987-b3d8-41a222155012",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76012f6f-e2f2-43e0-a743-a09e2ad10ce3",
    "_uuid": "54a405d5-2b9e-4c47-a1d9-710a8c2625f6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "01b27ab1-352f-4702-8f6a-1d02d53d6afa",
    "_uuid": "b8d6ef21-2db8-4fe7-93ab-d27796354908",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-20T11:53:47.726782Z",
     "iopub.status.busy": "2025-07-20T11:53:47.726582Z",
     "iopub.status.idle": "2025-07-20T11:53:47.784037Z",
     "shell.execute_reply": "2025-07-20T11:53:47.783282Z",
     "shell.execute_reply.started": "2025-07-20T11:53:47.726766Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "epochs = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "\n",
    "\n",
    "loss_fxn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.AdamW(custom_diff_model.parameters(), lr=lr)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"no\", gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS)\n",
    "custom_diff_model, optimizer, vae, text_encoder, train_loader = accelerator.prepare(custom_diff_model, optimizer, vae, text_encoder, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fdb2dadd-4374-4d55-9398-4a884d7f607e",
    "_uuid": "000283cd-0db8-495d-a9ee-a707fb8404ad",
    "collapsed": false,
    "execution": {
     "execution_failed": "2025-07-20T13:35:43.301Z",
     "iopub.execute_input": "2025-07-20T11:53:47.785108Z",
     "iopub.status.busy": "2025-07-20T11:53:47.784875Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Step 0, Loss: 130573.0343\n",
      "Epoch 1/2, Step 50, Loss: 1.2985\n",
      "Epoch 1/2, Step 100, Loss: 1.2966\n",
      "Epoch 1/2, Step 150, Loss: 1.2956\n",
      "Epoch 1/2, Step 200, Loss: 1.2874\n",
      "Epoch 1/2, Step 250, Loss: 1.2830\n",
      "Epoch 1/2, Step 300, Loss: 1.2781\n",
      "Epoch 1/2, Step 350, Loss: 1.2800\n",
      "Epoch 1/2, Step 400, Loss: 1.2768\n",
      "Epoch 1/2, Step 450, Loss: 1.2766\n",
      "Epoch 1/2, Step 500, Loss: 1.2771\n",
      "Epoch 1/2, Step 550, Loss: 1.2762\n",
      "Epoch 1/2, Step 600, Loss: 1.2758\n",
      "Epoch 1/2, Step 650, Loss: 1.2776\n",
      "Epoch 1/2, Step 700, Loss: 1.2775\n",
      "Epoch 1/2, Step 750, Loss: 1.2775\n",
      "Epoch 1/2, Step 800, Loss: 1.2778\n",
      "Epoch 1 finished. Average Loss: 1.2759\n",
      "Saved UNet checkpoint for epoch 1\n",
      "Epoch 2/2, Step 0, Loss: 122379.4937\n",
      "Epoch 2/2, Step 50, Loss: 1.2966\n",
      "Epoch 2/2, Step 100, Loss: 1.2730\n",
      "Epoch 2/2, Step 150, Loss: 1.2749\n"
     ]
    }
   ],
   "source": [
    "custom_diff_model.train()\n",
    "\n",
    "losses = []\n",
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "    \n",
    "        for step, (text_input_ids, cloth, cloth_mask, segmentation, agnostic, target_img) in enumerate(train_loader):\n",
    "            with accelerator.accumulate(custom_diff_model):\n",
    "                \n",
    "                text_embeddings = text_encoder(text_input_ids).last_hidden_state\n",
    "                # print(f\"text_embeddings: {text_embeddings}\")\n",
    "        \n",
    "                target_latents = vae.encode(target_img).latent_dist.sample()\n",
    "                target_latents = target_latents * vae.config.scaling_factor\n",
    "                # print(f\"target_latents: {target_latents}\")\n",
    "        \n",
    "                cloth_latents = vae.encode(cloth).latent_dist.sample()\n",
    "                cloth_latents = cloth_latents * vae.config.scaling_factor\n",
    "                # print(f\"cloth_latents: {cloth_latents}\")\n",
    "        \n",
    "                agnostic_latents = vae.encode(agnostic).latent_dist.sample()\n",
    "                agnostic_latents = agnostic_latents * vae.config.scaling_factor\n",
    "                # print(f\"agnostic: {agnostic_latents}\")\n",
    "        \n",
    "                noise = torch.randn_like(target_latents, device=device)\n",
    "                timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (target_latents.shape[0],), device=device)\n",
    "                # print(f\"noise: {noise}, timesteps: {timesteps}\")\n",
    "                noisy_latents = scheduler.add_noise(target_latents, noise, timesteps)\n",
    "                # print(f\"noisy_latents: {noisy_latents.shape} : {noisy_latents}\")\n",
    "                \n",
    "                latent_model_input = torch.cat([noisy_latents, cloth_latents, agnostic_latents], dim=1)\n",
    "                # print(f\"latent_model_input: {latent_model_input.shape} : {latent_model_input}\")\n",
    "    \n",
    "                model_pred = custom_diff_model(latent_model_input, timesteps, text_embeddings).sample\n",
    "                # print(f\"model_pred: {model_pred.shape} : {model_pred}\")\n",
    "                \n",
    "                loss = loss_fxn(model_pred.float(), noise.float())\n",
    "                # loss = loss.to(torch.float32)\n",
    "                \n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(custom_diff_model.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step()\n",
    "    \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "                if accelerator.is_main_process:\n",
    "                    if step % 50 == 0:\n",
    "                        accelerator.print(f\"Epoch {epoch+1}/{epochs}, Step {step}, Loss: {(total_loss / (step+0.00001)):.4f}\")\n",
    "                        losses.append(total_loss / (step+0.00001))\n",
    "                        \n",
    "    \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accelerator.print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "        # --- NEW: Save Checkpoint ---\n",
    "        if accelerator.is_main_process:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(custom_diff_model)\n",
    "            torch.save(unwrapped_model.state_dict(), f\"model_weights_epoch_{epoch+1}.pth\")\n",
    "            print(f\"Saved UNet checkpoint for epoch {epoch+1}\")\n",
    "    \n",
    "    accelerator.end_training()\n",
    "    print(\"\\nFine-tuning completed.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exited\")\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    custom_diff_model = accelerator.unwrap_model(custom_diff_model)\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# torch_dtype = torch.float16\n",
    "\n",
    "custom_diff_model = CustomUNet()\n",
    "custom_diff_model = custom_diff_model.to(torch_dtype).to(device)\n",
    "custom_diff_model.load_state_dict(torch.load('/kaggle/working/model_weights_epoch1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "cloth = Image.open(\"/kaggle/input/vton-hd/test/cloth/00006_00.jpg\")\n",
    "cloth_mask = Image.open(\"/kaggle/input/vton-hd/test/cloth-mask/00006_00.jpg\")\n",
    "segmentation = Image.open(\"/kaggle/input/vton-hd/test/image-parse-v3/00006_00.png\")\n",
    "agnostic = Image.open(\"/kaggle/input/vton-hd/test/agnostic-v3.2/00006_00.jpg\")\n",
    "\n",
    "cloth_tensor = transform(cloth).to(torch_dtype).to(device).unsqueeze(0)\n",
    "cloth_mask_tensor = mask_transform(cloth_mask).to(torch_dtype).to(device).unsqueeze(0)\n",
    "segmentation_tensor = mask_transform(segmentation).to(torch_dtype).to(device).unsqueeze(0)\n",
    "agnostic_tensor = transform(agnostic).to(torch_dtype).to(device).unsqueeze(0)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch_dtype).to(device)\n",
    "vae.requires_grad_(False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    cloth_latents = vae.encode(cloth_tensor).latent_dist.sample()\n",
    "    cloth_latents = cloth_latents * vae.config.scaling_factor\n",
    "    \n",
    "    agnostic_latents = vae.encode(agnostic_tensor).latent_dist.sample()\n",
    "    agnostic_latents = agnostic_latents * vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gemini_prompt = \"Describe the image in short. Include only necessary details about the cloth that the person is wearing such that a person is able to understand the overall appearance of the cloth. Start you sentence with: A person wearing .... against a plane white background. I will use this output for text conditioning of a diffusion model so make the output appropriate.\"\n",
    "\n",
    "result = gemini.generate_content([cloth, gemini_prompt])\n",
    "prompt = result.text\n",
    "negative_prompt = \"\"\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch_dtype).to(device)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_input_ids = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    conditional_embeddings = text_encoder(text_input_ids)[0]\n",
    "    \n",
    "    uncond_input_ids = tokenizer(negative_prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    unconditional_embeddings = text_encoder(uncond_input_ids)[0]\n",
    "\n",
    "text_embeddings = torch.cat([unconditional_embeddings, conditional_embeddings])\n",
    "print(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "guidance_scale = 8.0\n",
    "num_inference_steps = 50\n",
    "seed = 42\n",
    "generator = torch.Generator(device=device).manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "custom_diff_model.eval()\n",
    "\n",
    "latents = torch.randn(1, 4, 64, 64, generator=generator, device=device, dtype=torch_dtype)\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "\n",
    "for i, t in enumerate(scheduler.timesteps):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        scaled_latents = scheduler.scale_model_input(latents, t)\n",
    "    \n",
    "        latent_model_input = torch.cat([scaled_latents, cloth_latents, cloth_mask_tensor, segmentation_tensor, agnostic_latents], dim=1)\n",
    "    \n",
    "        latent_model_input = torch.cat([latent_model_input] * 2)\n",
    "    \n",
    "        noise_pred = custom_diff_model(latent_model_input, t, text_embeddings).sample\n",
    "        \n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    \n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        print(f\"  Step {i+1} (Timestep {t.item()}): Latents Min: {latents.min().item():.4f}, Max: {latents.max().item():.4f}, Mean: {latents.mean().item():.4f}, Std: {latents.std().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "latents = 1 / vae.config.scaling_factor * latents\n",
    "print(f\"Latents (before VAE decode) min: {latents.min().item():.4f}, max: {latents.max().item():.4f}, mean: {latents.mean().item():.4f}, std: {latents.std().item():.4f}\")\n",
    "\n",
    "\n",
    "image = vae.decode(latents).sample\n",
    "print(\"Image decoded.\")\n",
    "print(f\"Decoded image min: {image.min().item():.4f}, max: {image.max().item():.4f}, mean: {image.mean().item():.4f}\")\n",
    "\n",
    "\n",
    "image = torch.clamp(image, -1, 1)\n",
    "image = (image / 2 + 0.5)\n",
    "image = torch.clamp(image, 0, 1)\n",
    "img = image.squeeze(0)\n",
    "img = (img * 255).byte()\n",
    "img = img.permute(1, 2, 0).detach().cpu().numpy()\n",
    "img = Image.fromarray(img)\n",
    "print(\"Image prepared for display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6793521,
     "sourceId": 10926820,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7901409,
     "sourceId": 12517694,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
