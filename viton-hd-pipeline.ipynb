{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12527325,"sourceType":"datasetVersion","datasetId":7907953},{"sourceId":12533533,"sourceType":"datasetVersion","datasetId":7912217},{"sourceId":12535598,"sourceType":"datasetVersion","datasetId":7913722},{"sourceId":12535952,"sourceType":"datasetVersion","datasetId":7913978},{"sourceId":3848,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":2749,"modelId":324}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/GoGoDuck912/Self-Correction-Human-Parsing.git\n%cd Self-Correction-Human-Parsing\n\nimport os\nos.makedirs(\"/kaggle/working/human-parsing\", exist_ok=True)\nos.makedirs(\"/kaggle/working/cloth-mask\", exist_ok=True)\nos.makedirs(\"/kaggle/working/json_keypoints\", exist_ok=True)\n\n!pip install inplace-abn\n!pip install -r requirements.txt\n\nprint(\"Deleting outdated 'modules' folder...\")\n!rm -rf ./modules\nprint(\"Fixing import statement...\")\n!sed -i 's/from modules import InPlaceABNSync/from inplace_abn import InPlaceABNSync/' ./networks/AugmentCE2P.py\nprint(\"Fixing activation function name...\")\n!sed -i \"s/activation='none'/activation='identity'/g\" ./networks/AugmentCE2P.py\nprint(\"\\nAll fixes applied successfully!\")\n\n!pip install git+https://github.com/facebookresearch/segment-anything.git\n!pip install opencv-python matplotlib\n!pip install tfjs-graph-converter\n\n!git clone https://github.com/rwightman/posenet-pytorch\n%cd posenet-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:25:22.638245Z","iopub.execute_input":"2025-07-21T15:25:22.638542Z","iopub.status.idle":"2025-07-21T15:27:30.572782Z","shell.execute_reply.started":"2025-07-21T15:25:22.638516Z","shell.execute_reply":"2025-07-21T15:27:30.571907Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'Self-Correction-Human-Parsing' already exists and is not an empty directory.\n/kaggle/working/Self-Correction-Human-Parsing\nRequirement already satisfied: inplace-abn in /usr/local/lib/python3.11/dist-packages (1.1.0)\nCollecting opencv-python==4.4.0.46 (from -r requirements.txt (line 1))\n  Using cached opencv-python-4.4.0.46.tar.gz (88.9 MB)\n  Installing build dependencies ... \u001b[?25l  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m√ó\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[?25herror\n\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n\n\u001b[31m√ó\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n\u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\nDeleting outdated 'modules' folder...\nFixing import statement...\nFixing activation function name...\n\nAll fixes applied successfully!\nCollecting git+https://github.com/facebookresearch/segment-anything.git\n  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-uebptise\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-uebptise\n  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tfjs-graph-converter in /usr/local/lib/python3.11/dist-packages (1.6.3)\nRequirement already satisfied: tensorflowjs>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from tfjs-graph-converter) (4.22.0)\nRequirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (0.10.6)\nRequirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (6.5.2)\nRequirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (0.5.2)\nRequirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (0.5.1)\nRequirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (2.18.0)\nRequirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (2.18.0)\nRequirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (1.11.0)\nRequirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (1.17.0)\nRequirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (0.16.1)\nRequirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (23.2)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.26.4)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.1.1)\nRequirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.2.5)\nRequirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.11.16)\nRequirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.74)\nRequirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (14.0.0)\nRequirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (4.14.0)\nRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (6.0.2)\nRequirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.9)\nRequirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.4.1)\nRequirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.4.0)\nRequirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.15.3)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (18.1.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.32.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (75.2.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.1.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.73.1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.8.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.14.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.37.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.2.3)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.45.1)\nRequirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.1.1)\nRequirement already satisfied: ydf in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.9.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.16.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.6.15)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.19.2)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.1.3)\nRequirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.89)\nRequirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.12.2)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.6.0)\nRequirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (4.12.3)\nRequirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.20.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.2)\nRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.0.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.5.1)\nRequirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.23.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2024.2.0)\nfatal: destination path 'posenet-pytorch' already exists and is not an empty directory.\n/kaggle/working/Self-Correction-Human-Parsing/posenet-pytorch\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def parsing_map(image_dir, output_dir=\"/kaggle/working/human-parsing\"):\n    \"\"\"\n    Runs the human parsing script from the correct directory.\n    \"\"\"\n    # This command assumes you've already run the setup cell that clones the repo\n    # and installs dependencies.\n    %cd /kaggle/working/Self-Correction-Human-Parsing\n    \n    !python3 simple_extractor.py \\\n        --dataset lip \\\n        --model-restore /kaggle/input/lip-modellll/exp-schp-201908261155-lip.pth \\\n        --input-dir {image_dir} \\\n        --output-dir {output_dir}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:27:36.373889Z","iopub.execute_input":"2025-07-21T15:27:36.374500Z","iopub.status.idle":"2025-07-21T15:27:36.380439Z","shell.execute_reply.started":"2025-07-21T15:27:36.374468Z","shell.execute_reply":"2025-07-21T15:27:36.379570Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom segment_anything import sam_model_registry, SamPredictor\nimport os\n\ndef cloth_mask(cloth_path, output_dir=\"/kaggle/working/cloth-mask\"):\n    \"\"\"\n    Creates a segmentation mask for a single cloth image.\n    \"\"\"\n    # (SAM model loading logic is the same...)\n    sam_checkpoint = \"/kaggle/input/segment-anything/pytorch/vit-b/1/model.pth\"\n    model_type = \"vit_b\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)\n    predictor = SamPredictor(sam)\n\n    image = cv2.imread(cloth_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    predictor.set_image(image)\n    \n    h, w = image.shape[:2]\n    input_point = np.array([[w // 2, h // 2]])\n    input_label = np.array([1])\n    \n    masks, scores, _ = predictor.predict(\n        point_coords=input_point, point_labels=input_label, multimask_output=True)\n        \n    best_mask = (masks[np.argmax(scores)] * 255).astype(np.uint8)\n    \n    # <-- FIX: Create a dynamic output name.\n    os.makedirs(output_dir, exist_ok=True)\n    base_name = os.path.basename(cloth_path)\n    file_name_without_ext = os.path.splitext(base_name)[0]\n    output_filename = f\"{file_name_without_ext}_mask.png\"\n    output_path = os.path.join(output_dir, output_filename)\n    \n    cv2.imwrite(output_path, best_mask)\n    print(f\"Saved cloth mask to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:27:36.551781Z","iopub.execute_input":"2025-07-21T15:27:36.552183Z","iopub.status.idle":"2025-07-21T15:27:39.476928Z","shell.execute_reply.started":"2025-07-21T15:27:36.552158Z","shell.execute_reply":"2025-07-21T15:27:39.476357Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nimport posenet\n# --- FIX: Added the missing import for load_model ---\nfrom posenet.models.model_factory import load_model\n\ndef keypoints(image_path, output_dir=\"/kaggle/working/json_keypoints\"):\n    \"\"\"\n    Generates pose keypoints for a single image and saves them to a JSON file.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # This line will now work correctly\n    net = load_model(101).cuda()\n    output_stride = net.output_stride\n    \n    input_image, _, _ = posenet.read_imgfile(\n        image_path, scale_factor=1.0, output_stride=output_stride)\n\n    with torch.no_grad():\n        input_image = torch.Tensor(input_image).cuda()\n        heatmaps, offsets, fwd, bwd = net(input_image)\n        pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multiple_poses(\n            heatmaps.squeeze(0), offsets.squeeze(0), fwd.squeeze(0), bwd.squeeze(0),\n            output_stride=output_stride, max_pose_detections=1, min_pose_score=0.25)\n    \n    poses = keypoint_coords.astype(np.int32)\n    if not len(poses) > 0:\n        print(f\"Warning: No pose detected for {image_path}\")\n        return None\n\n    pose = poses[0]\n    \n    indices = [0, (5, 6), 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n    openpose = []\n    for ix in indices:\n        if ix == (5, 6):\n            openpose.append([int((pose[5][1] + pose[6][1]) / 2), int((pose[5][0] + pose[6][0]) / 2), 1])\n        else:\n            openpose.append([int(pose[ix][1]), int(pose[ix][0]), 1])\n\n    coords = [float(item) for sublist in openpose for item in sublist]\n    data = {\"people\": [{\"pose_keypoints_2d\": coords}]}\n    \n    base_name = os.path.basename(image_path)\n    file_name_without_ext = os.path.splitext(base_name)[0]\n    output_filename = f\"{file_name_without_ext}_keypoints.json\"\n    output_path = os.path.join(output_dir, output_filename)\n    \n    with open(output_path, 'w') as f:\n        json.dump(data, f)\n    \n    return output_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:27:39.478091Z","iopub.execute_input":"2025-07-21T15:27:39.478390Z","iopub.status.idle":"2025-07-21T15:27:39.490162Z","shell.execute_reply.started":"2025-07-21T15:27:39.478372Z","shell.execute_reply":"2025-07-21T15:27:39.489375Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import cv2\nimport json\nimport numpy as np\nimport os\n\ndef open_pose(image_path, keypoints_path, output_dir=\"/kaggle/working/openpose_image\"):\n    \"\"\"\n    Draws a pose skeleton on a blank canvas using keypoints from a JSON file.\n    Args:\n        image_path (str): Path to the original image to get canvas dimensions.\n        keypoints_path (str): Path to the JSON file containing the pose keypoints.\n        output_dir (str): Directory where the output skeleton image will be saved.\n    \"\"\"\n    # --- Load image using the function argument ---\n    img = cv2.imread(image_path)\n    if img is None:\n        print(f\"Error: Could not load image from {image_path}\")\n        return\n        \n    canvas_height, canvas_width = img.shape[:2]\n    canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)\n    \n    # --- Load keypoints using the function argument ---\n    with open(keypoints_path) as f:\n        keypoints_json = json.load(f)\n    \n    # Check if keypoints were found\n    if not keypoints_json['people']:\n        print(f\"Error: No people found in keypoints file {keypoints_path}\")\n        return\n\n    keypoints = np.array(keypoints_json['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n    \n    # --- Define pose pairs ---\n    POSE_PAIRS = [\n        (1, 2), (1, 5), (2, 3), (3, 4), (5, 6),\n        (6, 7), (1, 8), (8, 9), (9, 10), (1, 11),\n        (11, 12), (12, 13), (0, 1), (0, 14),\n        (14, 16), (0, 15), (15, 17)\n    ]\n    \n    # --- Color gradient for limbs (from red to violet) ---\n    def get_color(index, total):\n        hsv = np.array([[[int(index / total * 180), 255, 255]]], dtype=np.uint8)\n        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)[0][0]\n        return tuple(int(c) for c in bgr)\n    \n    # --- Draw keypoints and colored limbs ---\n    for i, (x, y, conf) in enumerate(keypoints):\n        if conf > 0.1:\n            cv2.circle(canvas, (int(x), int(y)), 5, (255, 255, 255), -1)  # white joint\n            # The line that draws the point numbers has been removed from here.\n    \n    for i, (partA, partB) in enumerate(POSE_PAIRS):\n        # This check is needed in case a keypoint is missing from the JSON\n        if partA < len(keypoints) and partB < len(keypoints):\n            xA, yA, cA = keypoints[partA]\n            xB, yB, cB = keypoints[partB]\n            if cA > 0.1 and cB > 0.1:\n                ptA = (int(xA), int(yA))\n                ptB = (int(xB), int(yB))\n                color = get_color(i, len(POSE_PAIRS))  # rainbow color\n                cv2.line(canvas, ptA, ptB, color, thickness=10)\n    \n    # --- Save the final skeleton image ---\n    os.makedirs(output_dir, exist_ok=True)\n    # Create a dynamic output name based on the input image\n    base_name = os.path.basename(image_path)\n    file_name_without_ext = os.path.splitext(base_name)[0]\n    output_filename = f\"{file_name_without_ext}_skeleton.png\"\n    output_path = os.path.join(output_dir, output_filename)\n    \n    cv2.imwrite(output_path, canvas)\n    print(f\"Skeleton image saved to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:27:39.491050Z","iopub.execute_input":"2025-07-21T15:27:39.491480Z","iopub.status.idle":"2025-07-21T15:27:39.512942Z","shell.execute_reply.started":"2025-07-21T15:27:39.491456Z","shell.execute_reply":"2025-07-21T15:27:39.512302Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\n\ndef pre_processing(cloth_dir, image_dir):\n    \"\"\"\n    Runs the full pre-processing pipeline by pairing sorted files from\n    the cloth and image directories.\n    \"\"\"\n    print(\"--- Starting Full Pre-processing Pipeline ---\")\n    \n    # Step 1: Run parsing on the entire image directory\n    print(\"\\n[1/4] Running human parsing on all images...\")\n    parsing_map(image_dir) \n    \n    # --- FIX: Added '.jpeg' to the list of accepted file extensions ---\n    image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n    cloth_files = sorted([f for f in os.listdir(cloth_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n\n    # Check if the directories have the same number of images\n    if len(image_files) != len(cloth_files):\n        print(f\" Warning: Mismatch in file counts. Images: {len(image_files)}, Cloths: {len(cloth_files)}\")\n        print(\"Processing only the minimum number of pairs.\")\n        \n    num_pairs = min(len(image_files), len(cloth_files))\n    \n    # Loop through the number of pairs\n    for i in range(num_pairs):\n        image_filename = image_files[i]\n        cloth_filename = cloth_files[i]\n        \n        print(f\"\\n--- Processing Pair {i+1}/{num_pairs}: ---\")\n        print(f\"  Image: {image_filename}\")\n        print(f\"  Cloth: {cloth_filename}\")\n        \n        # Define paths for the current pair\n        image_path = os.path.join(image_dir, image_filename)\n        cloth_path = os.path.join(cloth_dir, cloth_filename)\n\n        # Step 2: Create cloth mask\n        print(f\"[2/4] Creating cloth mask for {cloth_filename}...\")\n        cloth_mask(cloth_path)\n\n        # Step 3: Create keypoints JSON\n        print(f\"[3/4] Creating keypoints for {image_filename}...\")\n        keypoints_json_path = keypoints(image_path)\n        \n        if keypoints_json_path is None:\n            print(\"Skipping skeleton creation due to no keypoints.\")\n            continue\n\n        # Step 4: Create OpenPose skeleton image\n        print(f\"[4/4] Creating skeleton image for {image_filename}...\")\n        open_pose(image_path, keypoints_json_path)\n        \n    print(\"\\n--- Full Pre-processing Pipeline Finished! ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:27:39.599716Z","iopub.execute_input":"2025-07-21T15:27:39.599987Z","iopub.status.idle":"2025-07-21T15:27:39.607411Z","shell.execute_reply.started":"2025-07-21T15:27:39.599966Z","shell.execute_reply":"2025-07-21T15:27:39.606593Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!mkdir /kaggle/working/input_cloth\n!mkdir /kaggle/working/input_person","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:27:44.442627Z","iopub.execute_input":"2025-07-21T15:27:44.442933Z","iopub.status.idle":"2025-07-21T15:27:44.703132Z","shell.execute_reply.started":"2025-07-21T15:27:44.442910Z","shell.execute_reply":"2025-07-21T15:27:44.702351Z"}},"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‚Äò/kaggle/working/input_cloth‚Äô: File exists\nmkdir: cannot create directory ‚Äò/kaggle/working/input_person‚Äô: File exists\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# !cp /kaggle/input/test-data/00000_00.jpg /kaggle/working/input_person/00891_00.jpg\n# !cp /kaggle/input/test-data/00001_00.jpg /kaggle/working/input_cloth/01430_00.jpg\n\n\n\n!cp /kaggle/input/test-data2/00891_00.jpg /kaggle/working/input_person/00891_00.jpg\n!cp /kaggle/input/test-data2/01430_00.jpg /kaggle/working/input_cloth/01430_00.jpg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:27:44.727743Z","iopub.execute_input":"2025-07-21T15:27:44.728336Z","iopub.status.idle":"2025-07-21T15:27:44.995480Z","shell.execute_reply.started":"2025-07-21T15:27:44.728305Z","shell.execute_reply":"2025-07-21T15:27:44.994436Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define your input directories\nmy_cloth_directory = \"/kaggle/working/input_cloth\"\nmy_image_directory = \"/kaggle/working/input_person\"\n\n# Run the entire pipeline\npre_processing(cloth_dir=my_cloth_directory, image_dir=my_image_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:29:33.445135Z","iopub.execute_input":"2025-07-21T15:29:33.446131Z","iopub.status.idle":"2025-07-21T15:30:46.832571Z","shell.execute_reply.started":"2025-07-21T15:29:33.446095Z","shell.execute_reply":"2025-07-21T15:30:46.831582Z"}},"outputs":[{"name":"stdout","text":"--- Starting Full Pre-processing Pipeline ---\n\n[1/4] Running human parsing on all images...\n/kaggle/working/Self-Correction-Human-Parsing\nEvaluating total class number 20 with ['Background', 'Hat', 'Hair', 'Glove', 'Sunglasses', 'Upper-clothes', 'Dress', 'Coat', 'Socks', 'Pants', 'Jumpsuits', 'Scarf', 'Skirt', 'Face', 'Left-arm', 'Right-arm', 'Left-leg', 'Right-leg', 'Left-shoe', 'Right-shoe']\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.93it/s]\n\n--- Processing Pair 1/1: ---\n  Image: 00891_00.jpg\n  Cloth: 01430_00.jpg\n[2/4] Creating cloth mask for 01430_00.jpg...\nSaved cloth mask to /kaggle/working/cloth-mask/01430_00_mask.png\n[3/4] Creating keypoints for 00891_00.jpg...\nCannot find models file ./_models/mobilenet_v1_101.pth, converting from tfjs...\nWeights for checkpoint mobilenet_v1_101 are not downloaded. Downloading to /tmp/_posenet_weights ...\nDownloading MobilenetV1_Conv2d_0_biases\nDownloading MobilenetV1_Conv2d_0_weights\nDownloading MobilenetV1_Conv2d_10_depthwise_biases\nDownloading MobilenetV1_Conv2d_10_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_10_pointwise_biases\nDownloading MobilenetV1_Conv2d_10_pointwise_weights\nDownloading MobilenetV1_Conv2d_11_depthwise_biases\nDownloading MobilenetV1_Conv2d_11_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_11_pointwise_biases\nDownloading MobilenetV1_Conv2d_11_pointwise_weights\nDownloading MobilenetV1_Conv2d_12_depthwise_biases\nDownloading MobilenetV1_Conv2d_12_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_12_pointwise_biases\nDownloading MobilenetV1_Conv2d_12_pointwise_weights\nDownloading MobilenetV1_Conv2d_13_depthwise_biases\nDownloading MobilenetV1_Conv2d_13_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_13_pointwise_biases\nDownloading MobilenetV1_Conv2d_13_pointwise_weights\nDownloading MobilenetV1_Conv2d_1_depthwise_biases\nDownloading MobilenetV1_Conv2d_1_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_1_pointwise_biases\nDownloading MobilenetV1_Conv2d_1_pointwise_weights\nDownloading MobilenetV1_Conv2d_2_depthwise_biases\nDownloading MobilenetV1_Conv2d_2_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_2_pointwise_biases\nDownloading MobilenetV1_Conv2d_2_pointwise_weights\nDownloading MobilenetV1_Conv2d_3_depthwise_biases\nDownloading MobilenetV1_Conv2d_3_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_3_pointwise_biases\nDownloading MobilenetV1_Conv2d_3_pointwise_weights\nDownloading MobilenetV1_Conv2d_4_depthwise_biases\nDownloading MobilenetV1_Conv2d_4_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_4_pointwise_biases\nDownloading MobilenetV1_Conv2d_4_pointwise_weights\nDownloading MobilenetV1_Conv2d_5_depthwise_biases\nDownloading MobilenetV1_Conv2d_5_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_5_pointwise_biases\nDownloading MobilenetV1_Conv2d_5_pointwise_weights\nDownloading MobilenetV1_Conv2d_6_depthwise_biases\nDownloading MobilenetV1_Conv2d_6_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_6_pointwise_biases\nDownloading MobilenetV1_Conv2d_6_pointwise_weights\nDownloading MobilenetV1_Conv2d_7_depthwise_biases\nDownloading MobilenetV1_Conv2d_7_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_7_pointwise_biases\nDownloading MobilenetV1_Conv2d_7_pointwise_weights\nDownloading MobilenetV1_Conv2d_8_depthwise_biases\nDownloading MobilenetV1_Conv2d_8_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_8_pointwise_biases\nDownloading MobilenetV1_Conv2d_8_pointwise_weights\nDownloading MobilenetV1_Conv2d_9_depthwise_biases\nDownloading MobilenetV1_Conv2d_9_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_9_pointwise_biases\nDownloading MobilenetV1_Conv2d_9_pointwise_weights\nDownloading MobilenetV1_displacement_bwd_1_biases\nDownloading MobilenetV1_displacement_bwd_1_weights\nDownloading MobilenetV1_displacement_bwd_2_biases\nDownloading MobilenetV1_displacement_bwd_2_weights\nDownloading MobilenetV1_displacement_fwd_1_biases\nDownloading MobilenetV1_displacement_fwd_1_weights\nDownloading MobilenetV1_displacement_fwd_2_biases\nDownloading MobilenetV1_displacement_fwd_2_weights\nDownloading MobilenetV1_heatmap_1_biases\nDownloading MobilenetV1_heatmap_1_weights\nDownloading MobilenetV1_heatmap_2_biases\nDownloading MobilenetV1_heatmap_2_weights\nDownloading MobilenetV1_offset_1_biases\nDownloading MobilenetV1_offset_1_weights\nDownloading MobilenetV1_offset_2_biases\nDownloading MobilenetV1_offset_2_weights\nDownloading MobilenetV1_partheat_1_biases\nDownloading MobilenetV1_partheat_1_weights\nDownloading MobilenetV1_partheat_2_biases\nDownloading MobilenetV1_partheat_2_weights\nDownloading MobilenetV1_partoff_1_biases\nDownloading MobilenetV1_partoff_1_weights\nDownloading MobilenetV1_partoff_2_biases\nDownloading MobilenetV1_partoff_2_weights\nDownloading MobilenetV1_segment_1_biases\nDownloading MobilenetV1_segment_1_weights\nDownloading MobilenetV1_segment_2_biases\nDownloading MobilenetV1_segment_2_weights\n[4/4] Creating skeleton image for 00891_00.jpg...\nSkeleton image saved to: /kaggle/working/openpose_image/00891_00_skeleton.png\n\n--- Full Pre-processing Pipeline Finished! ---\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import cv2\nfrom PIL import Image\nimport torch\n\n!pip install torchgeometry\n!git clone https://github.com/shadow2496/VITON-HD.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp -r /kaggle/input/viton-hd-files/datasets/datasets/test/test /kaggle/working/VITON-HD/datasets\n!cp /kaggle/input/viton-hd-files/datasets/datasets/test_pairs.txt /kaggle/working/VITON-HD/test_pairs.txt\n!cp /kaggle/input/viton-hd-files/datasets/datasets/test_pairs.txt /kaggle/working/VITON-HD/datasets/test_pairs.txt\n\n!cp -r /kaggle/input/viton-hd-files/checkpoints/checkpoints /kaggle/working/VITON-HD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:44:36.199466Z","iopub.execute_input":"2025-07-21T15:44:36.199824Z","iopub.status.idle":"2025-07-21T15:44:37.995599Z","shell.execute_reply.started":"2025-07-21T15:44:36.199794Z","shell.execute_reply":"2025-07-21T15:44:37.994329Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"!cp /kaggle/working/cloth-mask/01430_00_mask.png /kaggle/working/VITON-HD/datasets/test/cloth-mask/01430_00.jpg\n!cp /kaggle/working/human-parsing/00891_00.png /kaggle/working/VITON-HD/datasets/test/image-parse/00891_00.png\n!cp /kaggle/working/json_keypoints/00891_00_keypoints.json /kaggle/working/VITON-HD/datasets/test/openpose-json/00891_00_keypoints.json\n!cp /kaggle/working/openpose_image/00891_00_skeleton.png /kaggle/working/VITON-HD/datasets/test/openpose-img/00891_00_rendered.png","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:41:08.928113Z","iopub.execute_input":"2025-07-21T15:41:08.928713Z","iopub.status.idle":"2025-07-21T15:41:09.382106Z","shell.execute_reply.started":"2025-07-21T15:41:08.928672Z","shell.execute_reply":"2025-07-21T15:41:09.380507Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"!cp /kaggle/working/input_person/00891_00.jpg /kaggle/working/VITON-HD/datasets/test/image/00891_00.jpg\n!cp /kaggle/working/input_cloth/01430_00.jpg /kaggle/working/VITON-HD/datasets/test/cloth/01430_00.jpg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/VITON-HD && CUDA_VISIBLE_DEVICES=0 python test.py --name VITON","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:41:42.886264Z","iopub.execute_input":"2025-07-21T15:41:42.887245Z","iopub.status.idle":"2025-07-21T15:41:53.936622Z","shell.execute_reply.started":"2025-07-21T15:41:42.887203Z","shell.execute_reply":"2025-07-21T15:41:53.935514Z"}},"outputs":[{"name":"stdout","text":"Namespace(name='VITON', batch_size=1, workers=1, load_height=1024, load_width=768, shuffle=False, dataset_dir='./datasets/', dataset_mode='test', dataset_list='test_pairs.txt', checkpoint_dir='./checkpoints/', save_dir='./results/', display_freq=1, seg_checkpoint='seg_final.pth', gmm_checkpoint='gmm_final.pth', alias_checkpoint='alias_final.pth', semantic_nc=13, init_type='xavier', init_variance=0.02, grid_size=5, norm_G='spectralaliasinstance', ngf=64, num_upsampling_layers='most')\nNetwork [SegGenerator] was created. Total number of parameters: 34.5 million. To see the architecture, do print(network).\nNetwork [ALIASGenerator] was created. Total number of parameters: 100.5 million. To see the architecture, do print(network).\n/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5015: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n  warnings.warn(\nstep: 1\nstep: 2\nstep: 3\nstep: 4\nstep: 5\nstep: 6\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!ls /kaggle/working/VITON-HD","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def VITON_HD(user_path, cloth_path, tryon_output_path):\n\n    user_image = Image.open(user_path)\n    cloth_image = Image.open(cloth_path)\n\n    my_cloth_directory = \"/kaggle/working/input_cloth\"\n    my_image_directory = \"/kaggle/working/input_person\"\n    \n    pre_processing(cloth_dir=my_cloth_directory, image_dir=my_image_directory)\n\n    !cp /kaggle/working/cloth-mask/01430_00_mask.png /kaggle/working/VITON-HD/datasets/test/cloth-mask/01430_00.jpg\n    !cp /kaggle/working/human-parsing/00891_00.png /kaggle/working/VITON-HD/datasets/test/image-parse/00891_00.png\n    !cp /kaggle/working/json_keypoints/00891_00_keypoints.json /kaggle/working/VITON-HD/datasets/test/openpose-json/00891_00_keypoints.json\n    # !cp /kaggle/working/openpose_image/00891_00_skeleton.png /kaggle/working/VITON-HD/datasets/test/openpose-img/00891_00_rendered.png\n\n    !cp /kaggle/working/input_person/00891_00.jpg /kaggle/working/VITON-HD/datasets/test/image/00891_00.jpg\n    !cp /kaggle/working/input_cloth/01430_00.jpg /kaggle/working/VITON-HD/datasets/test/cloth/01430_00.jpg\n\n    !cd /kaggle/working/VITON-HD && CUDA_VISIBLE_DEVICES=0 python test.py --name VITON\n\n    result = Image.open(\"/kaggle/working/VITON-HD/results/VITON/00891_01430_00.jpg\")\n    result.save(tryon_output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:07:42.993464Z","iopub.execute_input":"2025-07-21T16:07:42.993749Z","iopub.status.idle":"2025-07-21T16:07:43.005250Z","shell.execute_reply.started":"2025-07-21T16:07:42.993729Z","shell.execute_reply":"2025-07-21T16:07:43.004268Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"!pip install pyngrok\nfrom fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import FileResponse\nimport nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\nfrom PIL import Image\nimport os\n\napp = FastAPI()\n\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  #\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Make folders to store files\nos.makedirs(\"uploads\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\n\n@app.post(\"/vton\")\nasync def vton(cloth: UploadFile = File(...), user: UploadFile = File(...)):\n    # Save cloth and user images to disk\n    cloth_path = f\"uploads/{cloth.filename}\"\n    user_path = f\"uploads/{user.filename}\"\n    \n    with open(cloth_path, \"wb\") as f:\n        f.write(await cloth.read())\n    with open(user_path, \"wb\") as f:\n        f.write(await user.read())\n\n    # === Replace this line with your actual VTON code ===\n    tryon_output_path = \"results/tryon_output.png\"\n    VITON_HD(user_path, cloth_path, tryon_output_path)\n\n    # Return the result image back to the website\n    return FileResponse(tryon_output_path, media_type=\"image/png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:07:43.212491Z","iopub.execute_input":"2025-07-21T16:07:43.212753Z","iopub.status.idle":"2025-07-21T16:07:46.289827Z","shell.execute_reply.started":"2025-07-21T16:07:43.212736Z","shell.execute_reply":"2025-07-21T16:07:46.289066Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!ngrok config add-authtoken 30BEI0jrdnlCm5ePSwmrDlYyOmV_7AphnTKcssudeZhUaK5Yp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:07:46.291252Z","iopub.execute_input":"2025-07-21T16:07:46.291481Z","iopub.status.idle":"2025-07-21T16:07:46.588081Z","shell.execute_reply.started":"2025-07-21T16:07:46.291458Z","shell.execute_reply":"2025-07-21T16:07:46.586834Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"nest_asyncio.apply()\nport = 8000\npublic_url = ngrok.connect(port)\nprint(\"üåê Your public API:\", public_url)\nuvicorn.run(app, host=\"0.0.0.0\", port=port)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}