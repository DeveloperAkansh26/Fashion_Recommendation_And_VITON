{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12527325,"sourceType":"datasetVersion","datasetId":7907953},{"sourceId":12533533,"sourceType":"datasetVersion","datasetId":7912217},{"sourceId":12535598,"sourceType":"datasetVersion","datasetId":7913722},{"sourceId":12535952,"sourceType":"datasetVersion","datasetId":7913978},{"sourceId":3848,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":2749,"modelId":324}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/GoGoDuck912/Self-Correction-Human-Parsing.git\n%cd Self-Correction-Human-Parsing\n\nimport os\nos.makedirs(\"/kaggle/working/human-parsing\", exist_ok=True)\nos.makedirs(\"/kaggle/working/cloth-mask\", exist_ok=True)\nos.makedirs(\"/kaggle/working/json_keypoints\", exist_ok=True)\n\n!pip install inplace-abn\n!pip install -r requirements.txt\n\nprint(\"Deleting outdated 'modules' folder...\")\n!rm -rf ./modules\nprint(\"Fixing import statement...\")\n!sed -i 's/from modules import InPlaceABNSync/from inplace_abn import InPlaceABNSync/' ./networks/AugmentCE2P.py\nprint(\"Fixing activation function name...\")\n!sed -i \"s/activation='none'/activation='identity'/g\" ./networks/AugmentCE2P.py\nprint(\"\\nAll fixes applied successfully!\")\n\n!pip install git+https://github.com/facebookresearch/segment-anything.git\n!pip install opencv-python matplotlib\n!pip install tfjs-graph-converter\n\n!git clone https://github.com/rwightman/posenet-pytorch\n%cd posenet-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:59:15.530842Z","iopub.execute_input":"2025-08-06T17:59:15.531077Z","iopub.status.idle":"2025-08-06T18:02:24.601972Z","shell.execute_reply.started":"2025-08-06T17:59:15.531051Z","shell.execute_reply":"2025-08-06T18:02:24.600936Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Self-Correction-Human-Parsing'...\nremote: Enumerating objects: 722, done.\u001b[K\nremote: Counting objects: 100% (173/173), done.\u001b[K\nremote: Compressing objects: 100% (109/109), done.\u001b[K\nremote: Total 722 (delta 74), reused 64 (delta 64), pack-reused 549 (from 2)\u001b[K\nReceiving objects: 100% (722/722), 3.88 MiB | 30.34 MiB/s, done.\nResolving deltas: 100% (150/150), done.\n/kaggle/working/Self-Correction-Human-Parsing\nCollecting inplace-abn\n  Downloading inplace-abn-1.1.0.tar.gz (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: inplace-abn\n  Building wheel for inplace-abn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for inplace-abn: filename=inplace_abn-1.1.0-cp311-cp311-linux_x86_64.whl size=4475945 sha256=98bcf882f15ecc6f2aa7e5b16844d22872eec7456d77a0398e5cfe6dc0ff1fca\n  Stored in directory: /root/.cache/pip/wheels/cc/15/b3/8def1885c3cd26efdb5785ea48dcfa7e6df81b7ba99296cc7c\nSuccessfully built inplace-abn\nInstalling collected packages: inplace-abn\nSuccessfully installed inplace-abn-1.1.0\nCollecting opencv-python==4.4.0.46 (from -r requirements.txt (line 1))\n  Downloading opencv-python-4.4.0.46.tar.gz (88.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n\n\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\nDeleting outdated 'modules' folder...\nFixing import statement...\nFixing activation function name...\n\nAll fixes applied successfully!\nCollecting git+https://github.com/facebookresearch/segment-anything.git\n  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-j06x9b33\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-j06x9b33\n  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nCollecting tfjs-graph-converter\n  Downloading tfjs_graph_converter-1.6.3-py3-none-any.whl.metadata (11 kB)\nCollecting tensorflowjs>=1.5.2 (from tfjs-graph-converter)\n  Downloading tensorflowjs-4.22.0-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (0.10.6)\nRequirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (6.5.2)\nRequirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (0.5.2)\nRequirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (0.5.1)\nRequirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (2.18.0)\nRequirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (2.18.0)\nRequirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (1.11.0)\nRequirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (1.17.0)\nRequirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs>=1.5.2->tfjs-graph-converter) (0.16.1)\nCollecting packaging~=23.1 (from tensorflowjs>=1.5.2->tfjs-graph-converter)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.26.4)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.1.1)\nRequirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.2.5)\nRequirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.11.16)\nRequirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.74)\nRequirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (14.0.0)\nRequirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (4.14.0)\nRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (6.0.2)\nRequirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.9)\nRequirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.4.1)\nRequirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.4.0)\nRequirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.15.3)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (18.1.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.32.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (75.2.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.1.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.73.1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.8.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.14.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.37.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.2.3)\nRequirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.45.1)\nRequirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.1.1)\nRequirement already satisfied: ydf in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.9.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.16.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.6.15)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.19.2)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.1.3)\nRequirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.89)\nRequirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.12.2)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.6.0)\nRequirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (4.12.3)\nRequirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.20.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.2)\nRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.0.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2025.5.1)\nRequirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (3.23.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->flax>=0.7.2->tensorflowjs>=1.5.2->tfjs-graph-converter) (2024.2.0)\nDownloading tfjs_graph_converter-1.6.3-py3-none-any.whl (33 kB)\nDownloading tensorflowjs-4.22.0-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, tensorflowjs, tfjs-graph-converter\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ndb-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed packaging-23.2 tensorflowjs-4.22.0 tfjs-graph-converter-1.6.3\nCloning into 'posenet-pytorch'...\nremote: Enumerating objects: 121, done.\u001b[K\nremote: Counting objects: 100% (6/6), done.\u001b[K\nremote: Compressing objects: 100% (6/6), done.\u001b[K\nremote: Total 121 (delta 2), reused 1 (delta 0), pack-reused 115 (from 1)\u001b[K\nReceiving objects: 100% (121/121), 37.79 KiB | 9.45 MiB/s, done.\nResolving deltas: 100% (64/64), done.\n/kaggle/working/Self-Correction-Human-Parsing/posenet-pytorch\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def parsing_map(image_dir, output_dir=\"/kaggle/working/human-parsing\"):\n    \"\"\"\n    Runs the human parsing script from the correct directory.\n    \"\"\"\n    # This command assumes you've already run the setup cell that clones the repo\n    # and installs dependencies.\n    %cd /kaggle/working/Self-Correction-Human-Parsing\n    \n    !python3 simple_extractor.py \\\n        --dataset lip \\\n        --model-restore /kaggle/input/lip-modellll/exp-schp-201908261155-lip.pth \\\n        --input-dir {image_dir} \\\n        --output-dir {output_dir}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:02:46.404009Z","iopub.execute_input":"2025-08-06T18:02:46.404291Z","iopub.status.idle":"2025-08-06T18:02:46.409491Z","shell.execute_reply.started":"2025-08-06T18:02:46.404271Z","shell.execute_reply":"2025-08-06T18:02:46.408769Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom segment_anything import sam_model_registry, SamPredictor\nimport os\n\ndef cloth_mask(cloth_path, output_dir=\"/kaggle/working/cloth-mask\"):\n    \"\"\"\n    Creates a segmentation mask for a single cloth image.\n    \"\"\"\n    # (SAM model loading logic is the same...)\n    sam_checkpoint = \"/kaggle/input/segment-anything/pytorch/vit-b/1/model.pth\"\n    model_type = \"vit_b\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device)\n    predictor = SamPredictor(sam)\n\n    image = cv2.imread(cloth_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    predictor.set_image(image)\n    \n    h, w = image.shape[:2]\n    input_point = np.array([[w // 2, h // 2]])\n    input_label = np.array([1])\n    \n    masks, scores, _ = predictor.predict(\n        point_coords=input_point, point_labels=input_label, multimask_output=True)\n        \n    best_mask = (masks[np.argmax(scores)] * 255).astype(np.uint8)\n    \n    # <-- FIX: Create a dynamic output name.\n    os.makedirs(output_dir, exist_ok=True)\n    base_name = os.path.basename(cloth_path)\n    file_name_without_ext = os.path.splitext(base_name)[0]\n    output_filename = f\"{file_name_without_ext}_mask.png\"\n    output_path = os.path.join(output_dir, output_filename)\n    \n    cv2.imwrite(output_path, best_mask)\n    print(f\"Saved cloth mask to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:02:46.656930Z","iopub.execute_input":"2025-08-06T18:02:46.657190Z","iopub.status.idle":"2025-08-06T18:02:46.663948Z","shell.execute_reply.started":"2025-08-06T18:02:46.657171Z","shell.execute_reply":"2025-08-06T18:02:46.663235Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nimport posenet\n# --- FIX: Added the missing import for load_model ---\nfrom posenet.models.model_factory import load_model\n\ndef keypoints(image_path, output_dir=\"/kaggle/working/json_keypoints\"):\n    \"\"\"\n    Generates pose keypoints for a single image and saves them to a JSON file.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # This line will now work correctly\n    net = load_model(101).cuda()\n    output_stride = net.output_stride\n    \n    input_image, _, _ = posenet.read_imgfile(\n        image_path, scale_factor=1.0, output_stride=output_stride)\n\n    with torch.no_grad():\n        input_image = torch.Tensor(input_image).cuda()\n        heatmaps, offsets, fwd, bwd = net(input_image)\n        pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multiple_poses(\n            heatmaps.squeeze(0), offsets.squeeze(0), fwd.squeeze(0), bwd.squeeze(0),\n            output_stride=output_stride, max_pose_detections=1, min_pose_score=0.25)\n    \n    poses = keypoint_coords.astype(np.int32)\n    if not len(poses) > 0:\n        print(f\"Warning: No pose detected for {image_path}\")\n        return None\n\n    pose = poses[0]\n    \n    indices = [0, (5, 6), 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n    openpose = []\n    for ix in indices:\n        if ix == (5, 6):\n            openpose.append([int((pose[5][1] + pose[6][1]) / 2), int((pose[5][0] + pose[6][0]) / 2), 1])\n        else:\n            openpose.append([int(pose[ix][1]), int(pose[ix][0]), 1])\n\n    coords = [float(item) for sublist in openpose for item in sublist]\n    data = {\"people\": [{\"pose_keypoints_2d\": coords}]}\n    \n    base_name = os.path.basename(image_path)\n    file_name_without_ext = os.path.splitext(base_name)[0]\n    output_filename = f\"{file_name_without_ext}_keypoints.json\"\n    output_path = os.path.join(output_dir, output_filename)\n    \n    with open(output_path, 'w') as f:\n        json.dump(data, f)\n    \n    return output_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:02:48.568452Z","iopub.execute_input":"2025-08-06T18:02:48.568763Z","iopub.status.idle":"2025-08-06T18:02:48.586192Z","shell.execute_reply.started":"2025-08-06T18:02:48.568740Z","shell.execute_reply":"2025-08-06T18:02:48.585571Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import cv2\nimport json\nimport numpy as np\nimport os\n\ndef open_pose(image_path, keypoints_path, output_dir=\"/kaggle/working/openpose_image\"):\n    \"\"\"\n    Draws a pose skeleton on a blank canvas using keypoints from a JSON file.\n    Args:\n        image_path (str): Path to the original image to get canvas dimensions.\n        keypoints_path (str): Path to the JSON file containing the pose keypoints.\n        output_dir (str): Directory where the output skeleton image will be saved.\n    \"\"\"\n    # --- Load image using the function argument ---\n    img = cv2.imread(image_path)\n    if img is None:\n        print(f\"Error: Could not load image from {image_path}\")\n        return\n        \n    canvas_height, canvas_width = img.shape[:2]\n    canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)\n    \n    # --- Load keypoints using the function argument ---\n    with open(keypoints_path) as f:\n        keypoints_json = json.load(f)\n    \n    # Check if keypoints were found\n    if not keypoints_json['people']:\n        print(f\"Error: No people found in keypoints file {keypoints_path}\")\n        return\n\n    keypoints = np.array(keypoints_json['people'][0]['pose_keypoints_2d']).reshape(-1, 3)\n    \n    # --- Define pose pairs ---\n    POSE_PAIRS = [\n        (1, 2), (1, 5), (2, 3), (3, 4), (5, 6),\n        (6, 7), (1, 8), (8, 9), (9, 10), (1, 11),\n        (11, 12), (12, 13), (0, 1), (0, 14),\n        (14, 16), (0, 15), (15, 17)\n    ]\n    \n    # --- Color gradient for limbs (from red to violet) ---\n    def get_color(index, total):\n        hsv = np.array([[[int(index / total * 180), 255, 255]]], dtype=np.uint8)\n        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)[0][0]\n        return tuple(int(c) for c in bgr)\n    \n    # --- Draw keypoints and colored limbs ---\n    for i, (x, y, conf) in enumerate(keypoints):\n        if conf > 0.1:\n            cv2.circle(canvas, (int(x), int(y)), 5, (255, 255, 255), -1)  # white joint\n            # The line that draws the point numbers has been removed from here.\n    \n    for i, (partA, partB) in enumerate(POSE_PAIRS):\n        # This check is needed in case a keypoint is missing from the JSON\n        if partA < len(keypoints) and partB < len(keypoints):\n            xA, yA, cA = keypoints[partA]\n            xB, yB, cB = keypoints[partB]\n            if cA > 0.1 and cB > 0.1:\n                ptA = (int(xA), int(yA))\n                ptB = (int(xB), int(yB))\n                color = get_color(i, len(POSE_PAIRS))  # rainbow color\n                cv2.line(canvas, ptA, ptB, color, thickness=10)\n    \n    # --- Save the final skeleton image ---\n    os.makedirs(output_dir, exist_ok=True)\n    # Create a dynamic output name based on the input image\n    base_name = os.path.basename(image_path)\n    file_name_without_ext = os.path.splitext(base_name)[0]\n    output_filename = f\"{file_name_without_ext}_skeleton.png\"\n    output_path = os.path.join(output_dir, output_filename)\n    \n    cv2.imwrite(output_path, canvas)\n    print(f\"Skeleton image saved to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:02:50.186784Z","iopub.execute_input":"2025-08-06T18:02:50.187036Z","iopub.status.idle":"2025-08-06T18:02:50.198136Z","shell.execute_reply.started":"2025-08-06T18:02:50.187019Z","shell.execute_reply":"2025-08-06T18:02:50.197401Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\n\ndef pre_processing(cloth_dir, image_dir):\n    \"\"\"\n    Runs the full pre-processing pipeline by pairing sorted files from\n    the cloth and image directories.\n    \"\"\"\n    print(\"--- Starting Full Pre-processing Pipeline ---\")\n    \n    # Step 1: Run parsing on the entire image directory\n    print(\"\\n[1/4] Running human parsing on all images...\")\n    parsing_map(image_dir) \n    \n    # --- FIX: Added '.jpeg' to the list of accepted file extensions ---\n    image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n    cloth_files = sorted([f for f in os.listdir(cloth_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n\n    # Check if the directories have the same number of images\n    if len(image_files) != len(cloth_files):\n        print(f\" Warning: Mismatch in file counts. Images: {len(image_files)}, Cloths: {len(cloth_files)}\")\n        print(\"Processing only the minimum number of pairs.\")\n        \n    num_pairs = min(len(image_files), len(cloth_files))\n    \n    # Loop through the number of pairs\n    for i in range(num_pairs):\n        image_filename = image_files[i]\n        cloth_filename = cloth_files[i]\n        \n        print(f\"\\n--- Processing Pair {i+1}/{num_pairs}: ---\")\n        print(f\"  Image: {image_filename}\")\n        print(f\"  Cloth: {cloth_filename}\")\n        \n        # Define paths for the current pair\n        image_path = os.path.join(image_dir, image_filename)\n        cloth_path = os.path.join(cloth_dir, cloth_filename)\n\n        # Step 2: Create cloth mask\n        print(f\"[2/4] Creating cloth mask for {cloth_filename}...\")\n        cloth_mask(cloth_path)\n\n        # Step 3: Create keypoints JSON\n        print(f\"[3/4] Creating keypoints for {image_filename}...\")\n        keypoints_json_path = keypoints(image_path)\n        \n        if keypoints_json_path is None:\n            print(\"Skipping skeleton creation due to no keypoints.\")\n            continue\n\n        # Step 4: Create OpenPose skeleton image\n        print(f\"[4/4] Creating skeleton image for {image_filename}...\")\n        open_pose(image_path, keypoints_json_path)\n        \n    print(\"\\n--- Full Pre-processing Pipeline Finished! ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:02:51.645936Z","iopub.execute_input":"2025-08-06T18:02:51.646232Z","iopub.status.idle":"2025-08-06T18:02:51.653731Z","shell.execute_reply.started":"2025-08-06T18:02:51.646210Z","shell.execute_reply":"2025-08-06T18:02:51.652936Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!mkdir /kaggle/working/input_cloth\n!mkdir /kaggle/working/input_person","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:02:54.659104Z","iopub.execute_input":"2025-08-06T18:02:54.659364Z","iopub.status.idle":"2025-08-06T18:02:54.919476Z","shell.execute_reply.started":"2025-08-06T18:02:54.659347Z","shell.execute_reply":"2025-08-06T18:02:54.918418Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# !cp /kaggle/input/test-data/00000_00.jpg /kaggle/working/input_person/00891_00.jpg\n# !cp /kaggle/input/test-data/00001_00.jpg /kaggle/working/input_cloth/01430_00.jpg\n\n\n\n!cp /kaggle/input/test-data2/00891_00.jpg /kaggle/working/input_person/00891_00.jpg\n!cp /kaggle/input/test-data2/01430_00.jpg /kaggle/working/input_cloth/01430_00.jpg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:03:00.011059Z","iopub.execute_input":"2025-08-06T18:03:00.011664Z","iopub.status.idle":"2025-08-06T18:03:00.287881Z","shell.execute_reply.started":"2025-08-06T18:03:00.011632Z","shell.execute_reply":"2025-08-06T18:03:00.286840Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Define your input directories\nmy_cloth_directory = \"/kaggle/working/input_cloth\"\nmy_image_directory = \"/kaggle/working/input_person\"\n\n# Run the entire pipeline\npre_processing(cloth_dir=my_cloth_directory, image_dir=my_image_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:03:01.611158Z","iopub.execute_input":"2025-08-06T18:03:01.611462Z","iopub.status.idle":"2025-08-06T18:03:25.095851Z","shell.execute_reply.started":"2025-08-06T18:03:01.611436Z","shell.execute_reply":"2025-08-06T18:03:25.095097Z"}},"outputs":[{"name":"stdout","text":"--- Starting Full Pre-processing Pipeline ---\n\n[1/4] Running human parsing on all images...\n/kaggle/working/Self-Correction-Human-Parsing\nEvaluating total class number 20 with ['Background', 'Hat', 'Hair', 'Glove', 'Sunglasses', 'Upper-clothes', 'Dress', 'Coat', 'Socks', 'Pants', 'Jumpsuits', 'Scarf', 'Skirt', 'Face', 'Left-arm', 'Right-arm', 'Left-leg', 'Right-leg', 'Left-shoe', 'Right-shoe']\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n\n--- Processing Pair 1/1: ---\n  Image: 00891_00.jpg\n  Cloth: 01430_00.jpg\n[2/4] Creating cloth mask for 01430_00.jpg...\nSaved cloth mask to /kaggle/working/cloth-mask/01430_00_mask.png\n[3/4] Creating keypoints for 00891_00.jpg...\nCannot find models file ./_models/mobilenet_v1_101.pth, converting from tfjs...\nWeights for checkpoint mobilenet_v1_101 are not downloaded. Downloading to /tmp/_posenet_weights ...\nDownloading MobilenetV1_Conv2d_0_biases\nDownloading MobilenetV1_Conv2d_0_weights\nDownloading MobilenetV1_Conv2d_10_depthwise_biases\nDownloading MobilenetV1_Conv2d_10_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_10_pointwise_biases\nDownloading MobilenetV1_Conv2d_10_pointwise_weights\nDownloading MobilenetV1_Conv2d_11_depthwise_biases\nDownloading MobilenetV1_Conv2d_11_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_11_pointwise_biases\nDownloading MobilenetV1_Conv2d_11_pointwise_weights\nDownloading MobilenetV1_Conv2d_12_depthwise_biases\nDownloading MobilenetV1_Conv2d_12_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_12_pointwise_biases\nDownloading MobilenetV1_Conv2d_12_pointwise_weights\nDownloading MobilenetV1_Conv2d_13_depthwise_biases\nDownloading MobilenetV1_Conv2d_13_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_13_pointwise_biases\nDownloading MobilenetV1_Conv2d_13_pointwise_weights\nDownloading MobilenetV1_Conv2d_1_depthwise_biases\nDownloading MobilenetV1_Conv2d_1_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_1_pointwise_biases\nDownloading MobilenetV1_Conv2d_1_pointwise_weights\nDownloading MobilenetV1_Conv2d_2_depthwise_biases\nDownloading MobilenetV1_Conv2d_2_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_2_pointwise_biases\nDownloading MobilenetV1_Conv2d_2_pointwise_weights\nDownloading MobilenetV1_Conv2d_3_depthwise_biases\nDownloading MobilenetV1_Conv2d_3_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_3_pointwise_biases\nDownloading MobilenetV1_Conv2d_3_pointwise_weights\nDownloading MobilenetV1_Conv2d_4_depthwise_biases\nDownloading MobilenetV1_Conv2d_4_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_4_pointwise_biases\nDownloading MobilenetV1_Conv2d_4_pointwise_weights\nDownloading MobilenetV1_Conv2d_5_depthwise_biases\nDownloading MobilenetV1_Conv2d_5_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_5_pointwise_biases\nDownloading MobilenetV1_Conv2d_5_pointwise_weights\nDownloading MobilenetV1_Conv2d_6_depthwise_biases\nDownloading MobilenetV1_Conv2d_6_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_6_pointwise_biases\nDownloading MobilenetV1_Conv2d_6_pointwise_weights\nDownloading MobilenetV1_Conv2d_7_depthwise_biases\nDownloading MobilenetV1_Conv2d_7_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_7_pointwise_biases\nDownloading MobilenetV1_Conv2d_7_pointwise_weights\nDownloading MobilenetV1_Conv2d_8_depthwise_biases\nDownloading MobilenetV1_Conv2d_8_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_8_pointwise_biases\nDownloading MobilenetV1_Conv2d_8_pointwise_weights\nDownloading MobilenetV1_Conv2d_9_depthwise_biases\nDownloading MobilenetV1_Conv2d_9_depthwise_depthwise_weights\nDownloading MobilenetV1_Conv2d_9_pointwise_biases\nDownloading MobilenetV1_Conv2d_9_pointwise_weights\nDownloading MobilenetV1_displacement_bwd_1_biases\nDownloading MobilenetV1_displacement_bwd_1_weights\nDownloading MobilenetV1_displacement_bwd_2_biases\nDownloading MobilenetV1_displacement_bwd_2_weights\nDownloading MobilenetV1_displacement_fwd_1_biases\nDownloading MobilenetV1_displacement_fwd_1_weights\nDownloading MobilenetV1_displacement_fwd_2_biases\nDownloading MobilenetV1_displacement_fwd_2_weights\nDownloading MobilenetV1_heatmap_1_biases\nDownloading MobilenetV1_heatmap_1_weights\nDownloading MobilenetV1_heatmap_2_biases\nDownloading MobilenetV1_heatmap_2_weights\nDownloading MobilenetV1_offset_1_biases\nDownloading MobilenetV1_offset_1_weights\nDownloading MobilenetV1_offset_2_biases\nDownloading MobilenetV1_offset_2_weights\nDownloading MobilenetV1_partheat_1_biases\nDownloading MobilenetV1_partheat_1_weights\nDownloading MobilenetV1_partheat_2_biases\nDownloading MobilenetV1_partheat_2_weights\nDownloading MobilenetV1_partoff_1_biases\nDownloading MobilenetV1_partoff_1_weights\nDownloading MobilenetV1_partoff_2_biases\nDownloading MobilenetV1_partoff_2_weights\nDownloading MobilenetV1_segment_1_biases\nDownloading MobilenetV1_segment_1_weights\nDownloading MobilenetV1_segment_2_biases\nDownloading MobilenetV1_segment_2_weights\n[4/4] Creating skeleton image for 00891_00.jpg...\nSkeleton image saved to: /kaggle/working/openpose_image/00891_00_skeleton.png\n\n--- Full Pre-processing Pipeline Finished! ---\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import cv2\nfrom PIL import Image\nimport torch\n\n!pip install torchgeometry","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:04:08.266335Z","iopub.execute_input":"2025-08-06T18:04:08.266674Z","iopub.status.idle":"2025-08-06T18:05:17.693951Z","shell.execute_reply.started":"2025-08-06T18:04:08.266646Z","shell.execute_reply":"2025-08-06T18:05:17.692982Z"}},"outputs":[{"name":"stdout","text":"Collecting torchgeometry\n  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchgeometry) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->torchgeometry)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchgeometry) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->torchgeometry) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->torchgeometry) (3.0.2)\nDownloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchgeometry\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchgeometry-0.1.2\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:05:21.607630Z","iopub.execute_input":"2025-08-06T18:05:21.607929Z","iopub.status.idle":"2025-08-06T18:05:21.754740Z","shell.execute_reply.started":"2025-08-06T18:05:21.607901Z","shell.execute_reply":"2025-08-06T18:05:21.754052Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Self-Correction-Human-Parsing\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:05:22.680276Z","iopub.execute_input":"2025-08-06T18:05:22.680659Z","iopub.status.idle":"2025-08-06T18:05:22.686804Z","shell.execute_reply.started":"2025-08-06T18:05:22.680622Z","shell.execute_reply":"2025-08-06T18:05:22.686074Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!rm -r VITON-HD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:33:18.389012Z","iopub.execute_input":"2025-08-06T16:33:18.389366Z","iopub.status.idle":"2025-08-06T16:33:18.679631Z","shell.execute_reply.started":"2025-08-06T16:33:18.389336Z","shell.execute_reply":"2025-08-06T16:33:18.678544Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"!git clone https://github.com/shadow2496/VITON-HD.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:05:29.157124Z","iopub.execute_input":"2025-08-06T18:05:29.157794Z","iopub.status.idle":"2025-08-06T18:05:29.852321Z","shell.execute_reply.started":"2025-08-06T18:05:29.157758Z","shell.execute_reply":"2025-08-06T18:05:29.851635Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'VITON-HD'...\nremote: Enumerating objects: 52, done.\u001b[K\nremote: Counting objects: 100% (19/19), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 52 (delta 12), reused 8 (delta 7), pack-reused 33 (from 3)\u001b[K\nReceiving objects: 100% (52/52), 5.03 MiB | 38.15 MiB/s, done.\nResolving deltas: 100% (20/20), done.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!cp -r /kaggle/input/viton-hd-files/datasets/datasets/test/test /kaggle/working/VITON-HD/datasets\n!cp /kaggle/input/viton-hd-files/datasets/datasets/test_pairs.txt /kaggle/working/VITON-HD/test_pairs.txt\n!cp /kaggle/input/viton-hd-files/datasets/datasets/test_pairs.txt /kaggle/working/VITON-HD/datasets/test_pairs.txt\n\n!cp -r /kaggle/input/viton-hd-files/checkpoints/checkpoints /kaggle/working/VITON-HD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:05:32.366418Z","iopub.execute_input":"2025-08-06T18:05:32.366703Z","iopub.status.idle":"2025-08-06T18:05:38.724731Z","shell.execute_reply.started":"2025-08-06T18:05:32.366679Z","shell.execute_reply":"2025-08-06T18:05:38.723675Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"!cp /kaggle/working/cloth-mask/01430_00_mask.png /kaggle/working/VITON-HD/datasets/test/cloth-mask/01430_00.jpg\n!cp /kaggle/working/human-parsing/00891_00.png /kaggle/working/VITON-HD/datasets/test/image-parse/00891_00.png\n!cp /kaggle/working/json_keypoints/00891_00_keypoints.json /kaggle/working/VITON-HD/datasets/test/openpose-json/00891_00_keypoints.json\n!cp /kaggle/working/openpose_image/00891_00_skeleton.png /kaggle/working/VITON-HD/datasets/test/openpose-img/00891_00_rendered.png","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:05:38.726422Z","iopub.execute_input":"2025-08-06T18:05:38.726645Z","iopub.status.idle":"2025-08-06T18:05:39.280541Z","shell.execute_reply.started":"2025-08-06T18:05:38.726626Z","shell.execute_reply":"2025-08-06T18:05:39.279509Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"!cp /kaggle/working/input_person/00891_00.jpg /kaggle/working/VITON-HD/datasets/test/image/00891_00.jpg\n!cp /kaggle/working/input_cloth/01430_00.jpg /kaggle/working/VITON-HD/datasets/test/cloth/01430_00.jpg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:05:39.281782Z","iopub.execute_input":"2025-08-06T18:05:39.282049Z","iopub.status.idle":"2025-08-06T18:05:39.612509Z","shell.execute_reply.started":"2025-08-06T18:05:39.282026Z","shell.execute_reply":"2025-08-06T18:05:39.611667Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/VITON-HD && CUDA_VISIBLE_DEVICES=0 python test.py --name VITON","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:05:42.026532Z","iopub.execute_input":"2025-08-06T18:05:42.027350Z","iopub.status.idle":"2025-08-06T18:05:53.432401Z","shell.execute_reply.started":"2025-08-06T18:05:42.027307Z","shell.execute_reply":"2025-08-06T18:05:53.431713Z"}},"outputs":[{"name":"stdout","text":"Namespace(name='VITON', batch_size=1, workers=1, load_height=1024, load_width=768, shuffle=False, dataset_dir='./datasets/', dataset_mode='test', dataset_list='test_pairs.txt', checkpoint_dir='./checkpoints/', save_dir='./results/', display_freq=1, seg_checkpoint='seg_final.pth', gmm_checkpoint='gmm_final.pth', alias_checkpoint='alias_final.pth', semantic_nc=13, init_type='xavier', init_variance=0.02, grid_size=5, norm_G='spectralaliasinstance', ngf=64, num_upsampling_layers='most')\nNetwork [SegGenerator] was created. Total number of parameters: 34.5 million. To see the architecture, do print(network).\nNetwork [ALIASGenerator] was created. Total number of parameters: 100.5 million. To see the architecture, do print(network).\n/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5015: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n  warnings.warn(\nstep: 1\nstep: 2\nstep: 3\nstep: 4\nstep: 5\nstep: 6\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!ls /kaggle/working/VITON-HD","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def VITON_HD(user_path, cloth_path, tryon_output_path):\n\n    user_image = Image.open(user_path)\n    cloth_image = Image.open(cloth_path)\n\n    my_cloth_directory = \"/kaggle/working/input_cloth\"\n    my_image_directory = \"/kaggle/working/input_person\"\n    \n    pre_processing(cloth_dir=my_cloth_directory, image_dir=my_image_directory)\n\n    !cp /kaggle/working/cloth-mask/01430_00_mask.png /kaggle/working/VITON-HD/datasets/test/cloth-mask/01430_00.jpg\n    !cp /kaggle/working/human-parsing/00891_00.png /kaggle/working/VITON-HD/datasets/test/image-parse/00891_00.png\n    !cp /kaggle/working/json_keypoints/00891_00_keypoints.json /kaggle/working/VITON-HD/datasets/test/openpose-json/00891_00_keypoints.json\n    # !cp /kaggle/working/openpose_image/00891_00_skeleton.png /kaggle/working/VITON-HD/datasets/test/openpose-img/00891_00_rendered.png\n\n    !cp /kaggle/working/input_person/00891_00.jpg /kaggle/working/VITON-HD/datasets/test/image/00891_00.jpg\n    !cp /kaggle/working/input_cloth/01430_00.jpg /kaggle/working/VITON-HD/datasets/test/cloth/01430_00.jpg\n\n    !cd /kaggle/working/VITON-HD && CUDA_VISIBLE_DEVICES=0 python test.py --name VITON\n    \n    result = Image.open(\"/kaggle/working/VITON-HD/results/VITON/00891_01430_00.jpg\")\n\n    print(\"Saving to:\", tryon_output_path)\n    result.save(tryon_output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:05:56.927746Z","iopub.execute_input":"2025-08-06T18:05:56.928060Z","iopub.status.idle":"2025-08-06T18:05:56.940683Z","shell.execute_reply.started":"2025-08-06T18:05:56.928033Z","shell.execute_reply":"2025-08-06T18:05:56.939929Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"%cd /kaggle/working/VITON-HD","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:06:00.116582Z","iopub.execute_input":"2025-08-06T18:06:00.116879Z","iopub.status.idle":"2025-08-06T18:06:00.122165Z","shell.execute_reply.started":"2025-08-06T18:06:00.116856Z","shell.execute_reply":"2025-08-06T18:06:00.121528Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/VITON-HD\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:42:04.553498Z","iopub.execute_input":"2025-08-06T16:42:04.553806Z","iopub.status.idle":"2025-08-06T16:42:04.726534Z","shell.execute_reply.started":"2025-08-06T16:42:04.553785Z","shell.execute_reply":"2025-08-06T16:42:04.725524Z"}},"outputs":[{"name":"stdout","text":"assets\t     datasets\t  LICENSE      __pycache__  results\t    test.py\ncheckpoints  datasets.py  networks.py  README.md    test_pairs.txt  utils.py\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!pip install pyngrok\nfrom fastapi import FastAPI, File, UploadFile\nfrom fastapi.responses import FileResponse\nimport nest_asyncio\nfrom pyngrok import ngrok\nimport uvicorn\nfrom PIL import Image\nimport os\n\napp = FastAPI()\n\nfrom fastapi.middleware.cors import CORSMiddleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  #\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Make folders to store files\nos.makedirs(\"uploads\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\n\n@app.post(\"/vton\")\nasync def vton(cloth: UploadFile = File(...), user: UploadFile = File(...)):\n    # Save cloth and user images to disk\n    cloth_path = f\"uploads/{cloth.filename}\"\n    user_path = f\"uploads/{user.filename}\"\n    \n    with open(cloth_path, \"wb\") as f:\n        f.write(await cloth.read())\n    with open(user_path, \"wb\") as f:\n        f.write(await user.read())\n\n    # === Replace this line with your actual VTON code ===\n    tryon_output_path = \"results/tryon_output.png\"\n    VITON_HD(user_path, cloth_path, tryon_output_path)\n\n    # Return the result image back to the website\n    return FileResponse(tryon_output_path, media_type=\"image/png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:06:10.011514Z","iopub.execute_input":"2025-08-06T18:06:10.011822Z","iopub.status.idle":"2025-08-06T18:06:14.307330Z","shell.execute_reply.started":"2025-08-06T18:06:10.011797Z","shell.execute_reply":"2025-08-06T18:06:14.306465Z"}},"outputs":[{"name":"stdout","text":"Collecting pyngrok\n  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\nDownloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.3.0\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!ngrok config add-authtoken 30BEI0jrdnlCm5ePSwmrDlYyOmV_7AphnTKcssudeZhUaK5Yp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:06:14.308545Z","iopub.execute_input":"2025-08-06T18:06:14.308802Z","iopub.status.idle":"2025-08-06T18:06:15.643556Z","shell.execute_reply.started":"2025-08-06T18:06:14.308781Z","shell.execute_reply":"2025-08-06T18:06:15.642864Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"nest_asyncio.apply()\nport = 8000\npublic_url = ngrok.connect(port)\nprint(\"🌐 Your public API:\", public_url)\nuvicorn.run(app, host=\"0.0.0.0\", port=port)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T18:11:18.811846Z","iopub.execute_input":"2025-08-06T18:11:18.812662Z","execution_failed":"2025-08-06T20:19:02.854Z"}},"outputs":[{"name":"stderr","text":"INFO:     Started server process [36]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n","output_type":"stream"},{"name":"stdout","text":"🌐 Your public API: NgrokTunnel: \"https://ff3058896099.ngrok-free.app\" -> \"http://localhost:8000\"\nINFO:     103.237.157.57:0 - \"GET / HTTP/1.1\" 404 Not Found\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}