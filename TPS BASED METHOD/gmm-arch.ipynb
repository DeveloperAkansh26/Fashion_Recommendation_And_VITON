{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6683799,"sourceType":"datasetVersion","datasetId":3855472},{"sourceId":12520813,"sourceType":"datasetVersion","datasetId":7903411}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n        self.bn4 = nn.BatchNorm2d(512)\n        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.bn5 = nn.BatchNorm2d(512)\n        self.conv6 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.bn6 = nn.BatchNorm2d(512)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        x = F.relu(self.bn2(self.conv2(x)), inplace=True)\n        x = F.relu(self.bn3(self.conv3(x)), inplace=True)\n        x = F.relu(self.bn4(self.conv4(x)), inplace=True)\n        x = F.relu(self.bn5(self.conv5(x)), inplace=True)\n        x = F.relu(self.bn6(self.conv6(x)), inplace=True)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:56.394347Z","iopub.execute_input":"2025-07-20T02:36:56.394661Z","iopub.status.idle":"2025-07-20T02:36:56.402753Z","shell.execute_reply.started":"2025-07-20T02:36:56.394643Z","shell.execute_reply":"2025-07-20T02:36:56.402029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass FeatureCorrelation(nn.Module):\n    def __init__(self):\n        super(FeatureCorrelation, self).__init__()\n\n    def forward(self, feature_A, feature_B):\n        b, c, h, w = feature_A.size()\n\n        # reshape features for matrix multiplication\n        feature_A = feature_A.transpose(2, 3).contiguous().view(b, c, h * w)     # [B, C, HW]\n        feature_B = feature_B.view(b, c, h * w).transpose(1, 2)                  # [B, HW, C]\n\n        # perform matrix multiplication\n        feature_mul = torch.bmm(feature_B, feature_A)                            # [B, HW, HW]\n\n        # reshape to correlation tensor\n        correlation_tensor = feature_mul.view(b, h, w, h * w).transpose(2, 3).transpose(1, 2)  # [B, HW, HW] -> [B, h*w, h, w]\n\n        return correlation_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:56.404037Z","iopub.execute_input":"2025-07-20T02:36:56.404297Z","iopub.status.idle":"2025-07-20T02:36:56.417122Z","shell.execute_reply.started":"2025-07-20T02:36:56.404274Z","shell.execute_reply":"2025-07-20T02:36:56.416398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass FeatureRegression(nn.Module):\n    def __init__(self):\n        super(FeatureRegression, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(192, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        self.linear = nn.Linear(4 * 3 * 64, 18)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.contiguous().view(x.size(0), -1)\n        x = self.linear(x)\n        x = self.tanh(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:56.417846Z","iopub.execute_input":"2025-07-20T02:36:56.418489Z","iopub.status.idle":"2025-07-20T02:36:56.430867Z","shell.execute_reply.started":"2025-07-20T02:36:56.418441Z","shell.execute_reply":"2025-07-20T02:36:56.430238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TpsGridGen(nn.Module):\n    def __init__(self, out_h=256, out_w=192, use_regular_grid=True, grid_size=3, reg_factor=0, use_cuda=True):\n        super(TpsGridGen, self).__init__()\n        self.out_h, self.out_w = out_h, out_w\n        self.reg_factor = reg_factor\n        self.use_cuda = use_cuda\n\n        # create grid in numpy\n        self.grid = np.zeros( [self.out_h, self.out_w, 3], dtype=np.float32)\n        # sampling grid with dim-0 coords (Y)\n        self.grid_X,self.grid_Y = np.meshgrid(np.linspace(-1,1,out_w), np.linspace(-1,1,out_h))\n        # grid_X,grid_Y: size [1,H,W,1,1]\n        self.grid_X = torch.FloatTensor(self.grid_X).unsqueeze(0).unsqueeze(3)\n        self.grid_Y = torch.FloatTensor(self.grid_Y).unsqueeze(0).unsqueeze(3)\n        if use_cuda:\n            self.grid_X = self.grid_X.cuda()\n            self.grid_Y = self.grid_Y.cuda()\n\n        # initialize regular grid for control points P_i\n        if use_regular_grid:\n            axis_coords = np.linspace(-1,1,grid_size)\n            self.N = grid_size*grid_size\n            P_Y,P_X = np.meshgrid(axis_coords,axis_coords)\n            P_X = np.reshape(P_X,(-1,1)) # size (N,1)\n            P_Y = np.reshape(P_Y,(-1,1)) # size (N,1)\n            P_X = torch.FloatTensor(P_X)\n            P_Y = torch.FloatTensor(P_Y)\n            self.P_X_base = P_X.clone()\n            self.P_Y_base = P_Y.clone()\n            self.Li = self.compute_L_inverse(P_X,P_Y).unsqueeze(0)\n            self.P_X = P_X.unsqueeze(2).unsqueeze(3).unsqueeze(4).transpose(0,4)\n            self.P_Y = P_Y.unsqueeze(2).unsqueeze(3).unsqueeze(4).transpose(0,4)\n            if use_cuda:\n                self.P_X = self.P_X.cuda()\n                self.P_Y = self.P_Y.cuda()\n                self.P_X_base = self.P_X_base.cuda()\n                self.P_Y_base = self.P_Y_base.cuda()\n\n    def forward(self, theta):\n        warped_grid = self.apply_transformation(theta,torch.cat((self.grid_X,self.grid_Y),3))\n        return warped_grid\n\n    def compute_L_inverse(self,X,Y):\n        N = X.size()[0] # num of points (along dim 0)\n        # construct matrix K\n        Xmat = X.expand(N,N)\n        Ymat = Y.expand(N,N)\n        P_dist_squared = torch.pow(Xmat-Xmat.transpose(0,1),2)+torch.pow(Ymat-Ymat.transpose(0,1),2)\n        P_dist_squared[P_dist_squared==0]=1 # make diagonal 1 to avoid NaN in log computation\n        K = torch.mul(P_dist_squared,torch.log(P_dist_squared))\n        # construct matrix L\n        O = torch.FloatTensor(N,1).fill_(1)\n        Z = torch.FloatTensor(3,3).fill_(0)\n        P = torch.cat((O,X,Y),1)\n        L = torch.cat((torch.cat((K,P),1),torch.cat((P.transpose(0,1),Z),1)),0)\n        Li = torch.inverse(L)\n        if self.use_cuda:\n            Li = Li.cuda()\n        return Li\n\n    def apply_transformation(self,theta,points):\n        orig_device = theta.device\n        if self.use_cuda:\n            theta = theta.to(\"cuda:0\")\n        if theta.dim()==2:\n            theta = theta.unsqueeze(2).unsqueeze(3)\n        # points should be in the [B,H,W,2] format,\n        # where points[:,:,:,0] are the X coords  \n        # and points[:,:,:,1] are the Y coords  \n\n        # input are the corresponding control points P_i\n        batch_size = theta.size()[0]\n        # split theta into point coordinates\n        Q_X = theta[:,:self.N,:,:].squeeze(3)\n        Q_Y = theta[:,self.N:,:,:].squeeze(3)\n        Q_X = Q_X + self.P_X_base.expand_as(Q_X)\n        Q_Y = Q_Y + self.P_Y_base.expand_as(Q_Y)\n\n        # get spatial dimensions of points\n        points_b = points.size()[0]\n        points_h = points.size()[1]\n        points_w = points.size()[2]\n        # repeat pre-defined control points along spatial dimensions of points to be transformed\n        P_X = self.P_X.expand((1,points_h,points_w,1,self.N))\n        P_Y = self.P_Y.expand((1,points_h,points_w,1,self.N))\n\n        # compute weigths for non-linear part\n        W_X = torch.bmm(self.Li[:,:self.N,:self.N].expand((batch_size,self.N,self.N)),Q_X)\n        W_Y = torch.bmm(self.Li[:,:self.N,:self.N].expand((batch_size,self.N,self.N)),Q_Y)\n        # reshape\n        # W_X,W,Y: size [B,H,W,1,N]\n        W_X = W_X.unsqueeze(3).unsqueeze(4).transpose(1,4).repeat(1,points_h,points_w,1,1)\n        W_Y = W_Y.unsqueeze(3).unsqueeze(4).transpose(1,4).repeat(1,points_h,points_w,1,1)\n        # compute weights for affine part\n        A_X = torch.bmm(self.Li[:,self.N:,:self.N].expand((batch_size,3,self.N)),Q_X)\n        A_Y = torch.bmm(self.Li[:,self.N:,:self.N].expand((batch_size,3,self.N)),Q_Y)\n        # reshape\n        # A_X,A,Y: size [B,H,W,1,3]\n        A_X = A_X.unsqueeze(3).unsqueeze(4).transpose(1,4).repeat(1,points_h,points_w,1,1)\n        A_Y = A_Y.unsqueeze(3).unsqueeze(4).transpose(1,4).repeat(1,points_h,points_w,1,1)\n\n        # compute distance P_i - (grid_X,grid_Y)\n        # grid is expanded in point dim 4, but not in batch dim 0, as points P_X,P_Y are fixed for all batch\n        points_X_for_summation = points[:,:,:,0].unsqueeze(3).unsqueeze(4).expand(points[:,:,:,0].size()+(1,self.N))\n        points_Y_for_summation = points[:,:,:,1].unsqueeze(3).unsqueeze(4).expand(points[:,:,:,1].size()+(1,self.N))\n\n        if points_b==1:\n            delta_X = points_X_for_summation-P_X\n            delta_Y = points_Y_for_summation-P_Y\n        else:\n            # use expanded P_X,P_Y in batch dimension\n            delta_X = points_X_for_summation-P_X.expand_as(points_X_for_summation)\n            delta_Y = points_Y_for_summation-P_Y.expand_as(points_Y_for_summation)\n\n        dist_squared = torch.pow(delta_X,2)+torch.pow(delta_Y,2)\n        # U: size [1,H,W,1,N]\n        dist_squared[dist_squared==0]=1 # avoid NaN in log computation\n        U = torch.mul(dist_squared,torch.log(dist_squared)) \n\n        # expand grid in batch dimension if necessary\n        points_X_batch = points[:,:,:,0].unsqueeze(3)\n        points_Y_batch = points[:,:,:,1].unsqueeze(3)\n        if points_b==1:\n            points_X_batch = points_X_batch.expand((batch_size,)+points_X_batch.size()[1:])\n            points_Y_batch = points_Y_batch.expand((batch_size,)+points_Y_batch.size()[1:])\n\n        points_X_prime = A_X[:,:,:,:,0]+ \\\n                       torch.mul(A_X[:,:,:,:,1],points_X_batch) + \\\n                       torch.mul(A_X[:,:,:,:,2],points_Y_batch) + \\\n                       torch.sum(torch.mul(W_X,U.expand_as(W_X)),4)\n\n        points_Y_prime = A_Y[:,:,:,:,0]+ \\\n                       torch.mul(A_Y[:,:,:,:,1],points_X_batch) + \\\n                       torch.mul(A_Y[:,:,:,:,2],points_Y_batch) + \\\n                       torch.sum(torch.mul(W_Y,U.expand_as(W_Y)),4)\n\n        points_Y_prime = points_Y_prime.to(orig_device)\n        points_X_prime = points_X_prime.to(orig_device)\n\n        return torch.cat((points_X_prime,points_Y_prime),3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:56.550686Z","iopub.execute_input":"2025-07-20T02:36:56.550896Z","iopub.status.idle":"2025-07-20T02:36:56.570916Z","shell.execute_reply.started":"2025-07-20T02:36:56.550882Z","shell.execute_reply":"2025-07-20T02:36:56.570320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport os\nfrom PIL import Image\nfrom torchvision import transforms\n\nclass GMMDataset(Dataset):\n    def __init__(self, names):\n        self.names = names\n\n        self.segm_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/image-parse-v3'\n        self.person_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/image'\n        self.cloth_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/cloth'\n        self.deformed_dir = '/kaggle/input/deformed-cloth-test/deformed_cloth_test_png'\n\n        self.transform = transforms.Compose([\n            transforms.Resize((256, 192)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n        ])\n\n    def __len__(self):\n        return len(self.names)\n\n    def __getitem__(self, idx):\n        name = self.names[idx]\n\n        segm = self.transform(Image.open(os.path.join(self.segm_dir, name + '.png')).convert('RGB'))\n        person = self.transform(Image.open(os.path.join(self.person_dir, name + '.jpg')).convert('RGB'))\n        cloth = self.transform(Image.open(os.path.join(self.cloth_dir, name + '.jpg')).convert('RGB'))\n        y = self.transform(Image.open(os.path.join(self.deformed_dir, name + '.png')).convert('RGB'))\n\n        return segm, person, y, cloth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:56.572257Z","iopub.execute_input":"2025-07-20T02:36:56.572473Z","iopub.status.idle":"2025-07-20T02:36:56.584028Z","shell.execute_reply.started":"2025-07-20T02:36:56.572439Z","shell.execute_reply":"2025-07-20T02:36:56.583417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport glob\nimport os\n\n# Get all base names from image dir\nimage_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/image'\nall_image_paths = sorted(glob.glob(os.path.join(image_dir, '*.jpg')))\nall_names = [os.path.basename(p).split('.')[0] for p in all_image_paths]\n\n# Split: 80% train, 20% val\ntrain_names, val_names = train_test_split(all_names, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:56.584643Z","iopub.execute_input":"2025-07-20T02:36:56.584827Z","iopub.status.idle":"2025-07-20T02:36:56.608774Z","shell.execute_reply.started":"2025-07-20T02:36:56.584813Z","shell.execute_reply":"2025-07-20T02:36:56.608306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataset = GMMDataset(train_names)\nval_dataset = GMMDataset(val_names)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:56.609291Z","iopub.execute_input":"2025-07-20T02:36:56.609441Z","iopub.status.idle":"2025-07-20T02:36:56.613867Z","shell.execute_reply.started":"2025-07-20T02:36:56.609430Z","shell.execute_reply":"2025-07-20T02:36:56.613300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\nimport torchvision.utils as vutils\n\ndef denorm(tensor):\n    # Convert from [-1, 1] → [0, 1]\n    return tensor * 0.5 + 0.5\n\ndef show_batch(dataloader, title=\"Batch Samples\"):\n    batch = next(iter(dataloader))  # get first batch\n    segm, person, y, cloth = batch\n\n    # Pick a random index from batch\n    idx = random.randint(0, segm.size(0) - 1)\n\n    fig, axs = plt.subplots(1, 4, figsize=(8, 3))\n    axs[0].imshow(denorm(segm[idx]).permute(1, 2, 0).cpu().numpy())\n    axs[0].set_title(\"Segmentation Map (A)\")\n\n    axs[1].imshow(denorm(person[idx]).permute(1, 2, 0).cpu().numpy())\n    axs[1].set_title(\"Person (B)\")\n\n    axs[2].imshow(denorm(cloth[idx]).permute(1, 2, 0).cpu().numpy())\n    axs[2].set_title(\"Original Cloth (input)\")\n\n    axs[3].imshow(denorm(y[idx]).permute(1, 2, 0).cpu().numpy())\n    axs[3].set_title(\"Deformed Cloth (Ground Truth)\")\n\n    for ax in axs:\n        ax.axis(\"off\")\n\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\n# Example usage\nshow_batch(train_loader, \"Random Training Batch Sample\")\nshow_batch(val_loader, \"Random Validation Batch Sample\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:56.615715Z","iopub.execute_input":"2025-07-20T02:36:56.615929Z","iopub.status.idle":"2025-07-20T02:36:59.310806Z","shell.execute_reply.started":"2025-07-20T02:36:56.615914Z","shell.execute_reply":"2025-07-20T02:36:59.310111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def GMM(segm, person, y, cloth, feature_extractor, feature_correlation, feature_regressor):\n    # Step 1: Extract features\n    featureA = feature_extractor(segm)   # [B, C, H, W]\n    featureB = feature_extractor(person) # [B, C, H, W]\n\n    #  Step 2: Compute correlation\n    correlation = feature_correlation(featureA, featureB)  # [B, H*W, H, W]\n\n    #  Step 3: Predict TPS parameters\n    theta = feature_regressor(correlation)  # [B, 2N]\n\n    #  Step 4: Generate TPS grid\n    warped_grid = tps(theta)  # [B, H, W, 2]\n\n    # Step 5: Warp cloth\n    warped_cloth = F.grid_sample(cloth, warped_grid, padding_mode='border', align_corners=True)\n\n    return warped_cloth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:59.311685Z","iopub.execute_input":"2025-07-20T02:36:59.311936Z","iopub.status.idle":"2025-07-20T02:36:59.316695Z","shell.execute_reply.started":"2025-07-20T02:36:59.311904Z","shell.execute_reply":"2025-07-20T02:36:59.316032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn as nn\n\nclass Vgg19(nn.Module):\n    def __init__(self, requires_grad=False):\n        super(Vgg19, self).__init__()\n        vgg_pretrained_features = models.vgg19(pretrained=True).features\n        self.slice1 = nn.Sequential()\n        self.slice2 = nn.Sequential()\n        self.slice3 = nn.Sequential()\n        self.slice4 = nn.Sequential()\n        self.slice5 = nn.Sequential()\n        for x in range(2):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(2, 7):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(7, 12):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 21):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(21, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h_relu1 = self.slice1(X)\n        h_relu2 = self.slice2(h_relu1)\n        h_relu3 = self.slice3(h_relu2)\n        h_relu4 = self.slice4(h_relu3)\n        h_relu5 = self.slice5(h_relu4)\n        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:59.317266Z","iopub.execute_input":"2025-07-20T02:36:59.317516Z","iopub.status.idle":"2025-07-20T02:36:59.331469Z","shell.execute_reply.started":"2025-07-20T02:36:59.317498Z","shell.execute_reply":"2025-07-20T02:36:59.330751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass PerceptualLoss(nn.Module):\n    def __init__(self, vgg_model, weights=None):\n        super(PerceptualLoss, self).__init__()\n        self.vgg = vgg_model\n        self.weights = weights or [1.0, 1.0, 1.0, 1.0, 1.0]\n        self.criterion = nn.L1Loss()\n\n    def forward(self, x, y):\n        # Assume x and y are already in correct VGG input range (e.g., [0, 1])\n        x_vgg = self.vgg(x)\n        y_vgg = self.vgg(y)\n\n        loss = 0.0\n        for i in range(len(x_vgg)):\n            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i])\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:59.332277Z","iopub.execute_input":"2025-07-20T02:36:59.332527Z","iopub.status.idle":"2025-07-20T02:36:59.342517Z","shell.execute_reply.started":"2025-07-20T02:36:59.332509Z","shell.execute_reply":"2025-07-20T02:36:59.341885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nl1_loss_fn = nn.L1Loss()\nvgg_model = Vgg19().to(device)\nperceptual_loss_fn = PerceptualLoss(vgg_model).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:36:59.343305Z","iopub.execute_input":"2025-07-20T02:36:59.343933Z","iopub.status.idle":"2025-07-20T02:37:01.035029Z","shell.execute_reply.started":"2025-07-20T02:36:59.343915Z","shell.execute_reply":"2025-07-20T02:37:01.034473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\n\n# -----------------------\n# ✅ Setup\n# -----------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ✅ Instantiate and move submodules to device\nfeature_extractor = FeatureExtractor().to(device)\nfeature_correlation = FeatureCorrelation().to(device)\nfeature_regressor = FeatureRegression().to(device)\n\n# ✅ Initialize perceptual model\nvgg_model = Vgg19().to(device)\nperceptual_loss_fn = PerceptualLoss(vgg_model).to(device)\nl1_loss_fn = nn.L1Loss()\n\n# ✅ Combined loss function\ndef combined_loss(output, target, alpha=0.5):\n    l1 = l1_loss_fn(output, target)\n    perceptual = perceptual_loss_fn(output, target)\n    return alpha * l1 + (1 - alpha) * perceptual\n\n# -----------------------\n# ✅ Training Config\n# -----------------------\nnum_epochs = 50\npatience = 5\nearly_stop_counter = 0\nbest_val_loss = float('inf')\n\n# ✅ Optimizer\nparams = list(feature_extractor.parameters()) + \\\n         list(feature_correlation.parameters()) + \\\n         list(feature_regressor.parameters())\noptimizer = optim.Adam(params, lr=1e-4)\n\n# ✅ LR Scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n# -----------------------\n# ✅ Training Loop\n# -----------------------\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    # Train mode\n    feature_extractor.train()\n    feature_correlation.train()\n    feature_regressor.train()\n\n    train_loss = 0.0\n    for segm, person, y, cloth in train_loader:\n        segm, person, y, cloth = segm.to(device), person.to(device), y.to(device), cloth.to(device)\n\n        optimizer.zero_grad()\n        output = GMM(segm, person, y, cloth, feature_extractor, feature_correlation, feature_regressor)\n\n        # Rescale to [0, 1] if needed\n        output = (output + 1) / 2\n        y = (y + 1) / 2\n\n        loss = combined_loss(output, y)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * segm.size(0)\n\n    train_loss /= len(train_loader.dataset)\n    print(f\"Train Loss: {train_loss:.4f}\")\n\n    # Eval mode\n    feature_extractor.eval()\n    feature_correlation.eval()\n    feature_regressor.eval()\n\n    val_loss = 0.0\n    with torch.no_grad():\n        for segm, person, y, cloth in val_loader:\n            segm, person, y, cloth = segm.to(device), person.to(device), y.to(device), cloth.to(device)\n\n            output = GMM(segm, person, y, cloth, feature_extractor, feature_correlation, feature_regressor)\n            output = (output + 1) / 2\n            y = (y + 1) / 2\n\n            loss = combined_loss(output, y)\n            val_loss += loss.item() * segm.size(0)\n\n    val_loss /= len(val_loader.dataset)\n    print(f\"Validation Loss: {val_loss:.4f}\")\n\n    # Step scheduler\n    scheduler.step(val_loss)\n\n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stop_counter = 0\n        print(\"Validation loss improved. Saving model...\")\n\n        torch.save({\n            'feature_extractor': feature_extractor.state_dict(),\n            'feature_correlation': feature_correlation.state_dict(),\n            'feature_regressor': feature_regressor.state_dict(),\n        }, 'gmm_model.pth')\n    else:\n        early_stop_counter += 1\n        print(f\" No improvement. Early stopping counter: {early_stop_counter}/{patience}\")\n\n        if early_stop_counter >= patience:\n            print(\" Early stopping triggered.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T02:37:01.035792Z","iopub.execute_input":"2025-07-20T02:37:01.036051Z","iopub.status.idle":"2025-07-20T03:00:40.596248Z","shell.execute_reply.started":"2025-07-20T02:37:01.036030Z","shell.execute_reply":"2025-07-20T03:00:40.595514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\ndef visualize_predictions(feature_extractor, feature_correlation, feature_regressor, dataloader, num_samples=4):\n    feature_extractor.eval()\n    feature_correlation.eval()\n    feature_regressor.eval()\n\n    # Load from best checkpoint\n    checkpoint = torch.load('gmm_model.pth', map_location=device)\n    feature_extractor.load_state_dict(checkpoint['feature_extractor'])\n    feature_correlation.load_state_dict(checkpoint['feature_correlation'])\n    feature_regressor.load_state_dict(checkpoint['feature_regressor'])\n\n    with torch.no_grad():\n        batch = next(iter(dataloader))\n        segm, person, y, cloth = batch\n        segm, person, y, cloth = segm.to(device), person.to(device), y.to(device), cloth.to(device)\n\n        # Get predicted warped cloth\n        preds = GMM(segm, person, y, cloth, feature_extractor, feature_correlation, feature_regressor)\n\n        # Denormalize\n        segm = segm * 0.5 + 0.5\n        person = person * 0.5 + 0.5\n        cloth = cloth * 0.5 + 0.5\n        y = y * 0.5 + 0.5\n        preds = preds * 0.5 + 0.5\n\n        for i in range(min(num_samples, segm.size(0))):\n            fig, axs = plt.subplots(1, 5, figsize=(15, 4))\n\n            axs[0].imshow(segm[i].permute(1, 2, 0).cpu().numpy())\n            axs[0].set_title(\"Segmentation\")\n\n            axs[1].imshow(person[i].permute(1, 2, 0).cpu().numpy())\n            axs[1].set_title(\"Person\")\n\n            axs[2].imshow(cloth[i].permute(1, 2, 0).cpu().numpy())\n            axs[2].set_title(\"Original Cloth\")\n\n            axs[3].imshow(preds[i].permute(1, 2, 0).cpu().numpy())\n            axs[3].set_title(\"Warped Cloth (Predicted)\")\n\n            axs[4].imshow(y[i].permute(1, 2, 0).cpu().numpy())\n            axs[4].set_title(\"Ground Truth (Deformed)\")\n\n            for ax in axs:\n                ax.axis('off')\n            plt.tight_layout()\n            plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T03:02:20.035577Z","iopub.execute_input":"2025-07-20T03:02:20.035842Z","iopub.status.idle":"2025-07-20T03:02:20.044737Z","shell.execute_reply.started":"2025-07-20T03:02:20.035823Z","shell.execute_reply":"2025-07-20T03:02:20.044003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After training is done\nprint(\"\\n🔍 Visualizing Predictions from Validation Set:\")\nvisualize_predictions(feature_extractor, feature_correlation, feature_regressor, val_loader, num_samples=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T03:02:23.678860Z","iopub.execute_input":"2025-07-20T03:02:23.679677Z","iopub.status.idle":"2025-07-20T03:02:26.035520Z","shell.execute_reply.started":"2025-07-20T03:02:23.679655Z","shell.execute_reply":"2025-07-20T03:02:26.034703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load model weights\ncheckpoint = torch.load('gmm_model.pth', map_location=device)\n\nfeature_extractor.load_state_dict(checkpoint['feature_extractor'])\nfeature_correlation.load_state_dict(checkpoint['feature_correlation'])\nfeature_regressor.load_state_dict(checkpoint['feature_regressor'])\n\nfeature_extractor.to(device)\nfeature_correlation.to(device)\nfeature_regressor.to(device)\n\n# Loss functions\nl1_loss_fn = nn.L1Loss()\nvgg_model = Vgg19().to(device)\nperceptual_loss_fn = PerceptualLoss(vgg_model).to(device)\n\ndef combined_loss(output, target, alpha=0.2):\n    l1 = l1_loss_fn(output, target)\n    perceptual = perceptual_loss_fn(output, target)\n    return alpha * l1 + (1 - alpha) * perceptual\n\n# Training config\nstart_epoch = 22\nnum_epochs = 50\nbest_val_loss = float('inf')\n\n# Optimizer\nparams = list(feature_extractor.parameters()) + \\\n         list(feature_correlation.parameters()) + \\\n         list(feature_regressor.parameters())\noptimizer = optim.Adam(params, lr=1e-4)\n\n# Scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n\n# Resume training\nfor epoch in range(start_epoch, num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    feature_extractor.train()\n    feature_correlation.train()\n    feature_regressor.train()\n\n    train_loss = 0.0\n    for batch_idx, (segm, person, y, cloth) in enumerate(train_loader):\n        segm, person, y, cloth = segm.to(device), person.to(device), y.to(device), cloth.to(device)\n\n        optimizer.zero_grad()\n        output = GMM(segm, person, y, cloth, feature_extractor, feature_correlation, feature_regressor)\n\n        output = (output + 1) / 2\n        y = (y + 1) / 2\n\n        loss = combined_loss(output, y)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * segm.size(0)\n\n        if batch_idx % 10 == 0:\n            print(f\"Batch {batch_idx + 1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n\n    train_loss /= len(train_loader.dataset)\n    print(f\"Train Loss: {train_loss:.4f}\")\n\n    feature_extractor.eval()\n    feature_correlation.eval()\n    feature_regressor.eval()\n\n    val_loss = 0.0\n    with torch.no_grad():\n        for segm, person, y, cloth in val_loader:\n            segm, person, y, cloth = segm.to(device), person.to(device), y.to(device), cloth.to(device)\n\n            output = GMM(segm, person, y, cloth, feature_extractor, feature_correlation, feature_regressor)\n            output = (output + 1) / 2\n            y = (y + 1) / 2\n\n            loss = combined_loss(output, y)\n            val_loss += loss.item() * segm.size(0)\n\n    val_loss /= len(val_loader.dataset)\n    print(f\"Validation Loss: {val_loss:.4f}\")\n\n    scheduler.step(val_loss)\n\n    # Save model if validation improves\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        print(\"Validation loss improved. Saving model...\")\n        torch.save({\n            'feature_extractor': feature_extractor.state_dict(),\n            'feature_correlation': feature_correlation.state_dict(),\n            'feature_regressor': feature_regressor.state_dict(),\n        }, 'gmm_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T03:11:38.186870Z","iopub.execute_input":"2025-07-20T03:11:38.187585Z","iopub.status.idle":"2025-07-20T03:41:43.199098Z","shell.execute_reply.started":"2025-07-20T03:11:38.187554Z","shell.execute_reply":"2025-07-20T03:41:43.198278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\ndef visualize_predictions(feature_extractor, feature_correlation, feature_regressor, dataloader, num_samples=4):\n    feature_extractor.eval()\n    feature_correlation.eval()\n    feature_regressor.eval()\n\n    # Load from best checkpoint\n    checkpoint = torch.load('gmm_model.pth', map_location=device)\n    feature_extractor.load_state_dict(checkpoint['feature_extractor'])\n    feature_correlation.load_state_dict(checkpoint['feature_correlation'])\n    feature_regressor.load_state_dict(checkpoint['feature_regressor'])\n\n    with torch.no_grad():\n        batch = next(iter(dataloader))\n        segm, person, y, cloth = batch\n        segm, person, y, cloth = segm.to(device), person.to(device), y.to(device), cloth.to(device)\n\n        # Get predicted warped cloth\n        preds = GMM(segm, person, y, cloth, feature_extractor, feature_correlation, feature_regressor)\n\n        # Denormalize\n        segm = segm * 0.5 + 0.5\n        person = person * 0.5 + 0.5\n        cloth = cloth * 0.5 + 0.5\n        y = y * 0.5 + 0.5\n        preds = preds * 0.5 + 0.5\n\n        for i in range(min(num_samples, segm.size(0))):\n            fig, axs = plt.subplots(1, 5, figsize=(15, 4))\n\n            axs[0].imshow(segm[i].permute(1, 2, 0).cpu().numpy())\n            axs[0].set_title(\"Segmentation\")\n\n            axs[1].imshow(person[i].permute(1, 2, 0).cpu().numpy())\n            axs[1].set_title(\"Person\")\n\n            axs[2].imshow(cloth[i].permute(1, 2, 0).cpu().numpy())\n            axs[2].set_title(\"Original Cloth\")\n\n            axs[3].imshow(preds[i].permute(1, 2, 0).cpu().numpy())\n            axs[3].set_title(\"Warped Cloth (Predicted)\")\n\n            axs[4].imshow(y[i].permute(1, 2, 0).cpu().numpy())\n            axs[4].set_title(\"Ground Truth (Deformed)\")\n\n            for ax in axs:\n                ax.axis('off')\n            plt.tight_layout()\n            plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T03:41:43.200821Z","iopub.execute_input":"2025-07-20T03:41:43.201066Z","iopub.status.idle":"2025-07-20T03:41:43.211017Z","shell.execute_reply.started":"2025-07-20T03:41:43.201044Z","shell.execute_reply":"2025-07-20T03:41:43.210199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After training is done\nprint(\"\\n Visualizing Predictions from training Set:\")\nvisualize_predictions(feature_extractor, feature_correlation, feature_regressor, train_loader, num_samples=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T03:43:03.602625Z","iopub.execute_input":"2025-07-20T03:43:03.603267Z","iopub.status.idle":"2025-07-20T03:43:05.993128Z","shell.execute_reply.started":"2025-07-20T03:43:03.603241Z","shell.execute_reply":"2025-07-20T03:43:05.992501Z"}},"outputs":[],"execution_count":null}]}